arguments: .\create-dataset\align_dataset_mtcnn.py data/raw/ data/processed --image_size 112
--------------------
tensorflow version: 1.14.0
--------------------
git hash: b'e075462386ddff0aa1d0ca64a4975abb05713378'
--------------------
b'diff --git a/.gitattributes b/.gitattributes\ndeleted file mode 100644\nindex ec4a626..0000000\n--- a/.gitattributes\n+++ /dev/null\n@@ -1 +0,0 @@\n-*.pth filter=lfs diff=lfs merge=lfs -text\ndiff --git a/.gitignore b/.gitignore\ndeleted file mode 100644\nindex 894a44c..0000000\n--- a/.gitignore\n+++ /dev/null\n@@ -1,104 +0,0 @@\n-# Byte-compiled / optimized / DLL files\n-__pycache__/\n-*.py[cod]\n-*$py.class\n-\n-# C extensions\n-*.so\n-\n-# Distribution / packaging\n-.Python\n-build/\n-develop-eggs/\n-dist/\n-downloads/\n-eggs/\n-.eggs/\n-lib/\n-lib64/\n-parts/\n-sdist/\n-var/\n-wheels/\n-*.egg-info/\n-.installed.cfg\n-*.egg\n-MANIFEST\n-\n-# PyInstaller\n-#  Usually these files are written by a python script from a template\n-#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n-*.manifest\n-*.spec\n-\n-# Installer logs\n-pip-log.txt\n-pip-delete-this-directory.txt\n-\n-# Unit test / coverage reports\n-htmlcov/\n-.tox/\n-.coverage\n-.coverage.*\n-.cache\n-nosetests.xml\n-coverage.xml\n-*.cover\n-.hypothesis/\n-.pytest_cache/\n-\n-# Translations\n-*.mo\n-*.pot\n-\n-# Django stuff:\n-*.log\n-local_settings.py\n-db.sqlite3\n-\n-# Flask stuff:\n-instance/\n-.webassets-cache\n-\n-# Scrapy stuff:\n-.scrapy\n-\n-# Sphinx documentation\n-docs/_build/\n-\n-# PyBuilder\n-target/\n-\n-# Jupyter Notebook\n-.ipynb_checkpoints\n-\n-# pyenv\n-.python-version\n-\n-# celery beat schedule file\n-celerybeat-schedule\n-\n-# SageMath parsed files\n-*.sage.py\n-\n-# Environments\n-.env\n-.venv\n-env/\n-venv/\n-ENV/\n-env.bak/\n-venv.bak/\n-\n-# Spyder project settings\n-.spyderproject\n-.spyproject\n-\n-# Rope project settings\n-.ropeproject\n-\n-# mkdocs documentation\n-/site\n-\n-# mypy\n-.mypy_cache/\ndiff --git a/Learner.py b/Learner.py\ndeleted file mode 100644\nindex fc3ec6a..0000000\n--- a/Learner.py\n+++ /dev/null\n@@ -1,253 +0,0 @@\n-from data.data_pipe import de_preprocess, get_train_loader, get_val_data\n-from model import Backbone, Arcface, MobileFaceNet, Am_softmax, l2_norm\n-from verifacation import evaluate\n-import torch\n-from torch import optim\n-import numpy as np\n-from tqdm import tqdm\n-from tensorboardX import SummaryWriter\n-from matplotlib import pyplot as plt\n-plt.switch_backend(\'agg\')\n-from utils import get_time, gen_plot, hflip_batch, separate_bn_paras\n-from PIL import Image\n-from torchvision import transforms as trans\n-import math\n-import bcolz\n-\n-class face_learner(object):\n-    def __init__(self, conf, inference=False):\n-        print(conf)\n-        if conf.use_mobilfacenet:\n-            self.model = MobileFaceNet(conf.embedding_size).to(conf.device)\n-            print(\'MobileFaceNet model generated\')\n-        else:\n-            self.model = Backbone(conf.net_depth, conf.drop_ratio, conf.net_mode).to(conf.device)\n-            print(\'{}_{} model opened\'.format(conf.net_mode, conf.net_depth))\n-        \n-        if not inference:\n-            self.milestones = conf.milestones\n-            self.loader, self.class_num = get_train_loader(conf)        \n-\n-            self.writer = SummaryWriter(conf.log_path)\n-            self.step = 0\n-            self.head = Arcface(embedding_size=conf.embedding_size, classnum=self.class_num).to(conf.device)\n-\n-            print(\'two model heads generated\')\n-\n-            paras_only_bn, paras_wo_bn = separate_bn_paras(self.model)\n-            \n-            if conf.use_mobilfacenet:\n-                self.optimizer = optim.SGD([\n-                                    {\'params\': paras_wo_bn[:-1], \'weight_decay\': 4e-5},\n-                                    {\'params\': [paras_wo_bn[-1]] + [self.head.kernel], \'weight_decay\': 4e-4},\n-                                    {\'params\': paras_only_bn}\n-                                ], lr = conf.lr, momentum = conf.momentum)\n-            else:\n-                self.optimizer = optim.SGD([\n-                                    {\'params\': paras_wo_bn + [self.head.kernel], \'weight_decay\': 5e-4},\n-                                    {\'params\': paras_only_bn}\n-                                ], lr = conf.lr, momentum = conf.momentum)\n-            print(self.optimizer)\n-#             self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=40, verbose=True)\n-\n-            print(\'optimizers generated\')    \n-            self.board_loss_every = len(self.loader)//100\n-            self.evaluate_every = len(self.loader)//10\n-            self.save_every = len(self.loader)//5\n-            self.agedb_30, self.cfp_fp, self.lfw, self.agedb_30_issame, self.cfp_fp_issame, self.lfw_issame = get_val_data(self.loader.dataset.root.parent)\n-        else:\n-            self.threshold = conf.threshold\n-    \n-    def save_state(self, conf, accuracy, to_save_folder=False, extra=None, model_only=False):\n-        if to_save_folder:\n-            save_path = conf.save_path\n-        else:\n-            save_path = conf.model_path\n-        torch.save(\n-            self.model.state_dict(), save_path /\n-            (\'model_{}_accuracy:{}_step:{}_{}.pth\'.format(get_time(), accuracy, self.step, extra)))\n-        if not model_only:\n-            torch.save(\n-                self.head.state_dict(), save_path /\n-                (\'head_{}_accuracy:{}_step:{}_{}.pth\'.format(get_time(), accuracy, self.step, extra)))\n-            torch.save(\n-                self.optimizer.state_dict(), save_path /\n-                (\'optimizer_{}_accuracy:{}_step:{}_{}.pth\'.format(get_time(), accuracy, self.step, extra)))\n-    \n-    def load_state(self, conf, fixed_str, from_save_folder=False, model_only=False):\n-        if from_save_folder:\n-            save_path = conf.save_path\n-        else:\n-            save_path = conf.model_path            \n-        self.model.load_state_dict(torch.load(save_path/\'model_{}\'.format(fixed_str)))\n-        if not model_only:\n-            self.head.load_state_dict(torch.load(save_path/\'head_{}\'.format(fixed_str)))\n-            self.optimizer.load_state_dict(torch.load(save_path/\'optimizer_{}\'.format(fixed_str)))\n-        \n-    def board_val(self, db_name, accuracy, best_threshold, roc_curve_tensor):\n-        self.writer.add_scalar(\'{}_accuracy\'.format(db_name), accuracy, self.step)\n-        self.writer.add_scalar(\'{}_best_threshold\'.format(db_name), best_threshold, self.step)\n-        self.writer.add_image(\'{}_roc_curve\'.format(db_name), roc_curve_tensor, self.step)\n-#         self.writer.add_scalar(\'{}_val:true accept ratio\'.format(db_name), val, self.step)\n-#         self.writer.add_scalar(\'{}_val_std\'.format(db_name), val_std, self.step)\n-#         self.writer.add_scalar(\'{}_far:False Acceptance Ratio\'.format(db_name), far, self.step)\n-        \n-    def evaluate(self, conf, carray, issame, nrof_folds = 5, tta = False):\n-        self.model.eval()\n-        idx = 0\n-        embeddings = np.zeros([len(carray), conf.embedding_size])\n-        with torch.no_grad():\n-            while idx + conf.batch_size <= len(carray):\n-                batch = torch.tensor(carray[idx:idx + conf.batch_size])\n-                if tta:\n-                    fliped = hflip_batch(batch)\n-                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\n-                    embeddings[idx:idx + conf.batch_size] = l2_norm(emb_batch)\n-                else:\n-                    embeddings[idx:idx + conf.batch_size] = self.model(batch.to(conf.device)).cpu()\n-                idx += conf.batch_size\n-            if idx < len(carray):\n-                batch = torch.tensor(carray[idx:])            \n-                if tta:\n-                    fliped = hflip_batch(batch)\n-                    emb_batch = self.model(batch.to(conf.device)) + self.model(fliped.to(conf.device))\n-                    embeddings[idx:] = l2_norm(emb_batch)\n-                else:\n-                    embeddings[idx:] = self.model(batch.to(conf.device)).cpu()\n-        tpr, fpr, accuracy, best_thresholds = evaluate(embeddings, issame, nrof_folds)\n-        buf = gen_plot(fpr, tpr)\n-        roc_curve = Image.open(buf)\n-        roc_curve_tensor = trans.ToTensor()(roc_curve)\n-        return accuracy.mean(), best_thresholds.mean(), roc_curve_tensor\n-    \n-    def find_lr(self,\n-                conf,\n-                init_value=1e-8,\n-                final_value=10.,\n-                beta=0.98,\n-                bloding_scale=3.,\n-                num=None):\n-        if not num:\n-            num = len(self.loader)\n-        mult = (final_value / init_value)**(1 / num)\n-        lr = init_value\n-        for params in self.optimizer.param_groups:\n-            params[\'lr\'] = lr\n-        self.model.train()\n-        avg_loss = 0.\n-        best_loss = 0.\n-        batch_num = 0\n-        losses = []\n-        log_lrs = []\n-        for i, (imgs, labels) in tqdm(enumerate(self.loader), total=num):\n-\n-            imgs = imgs.to(conf.device)\n-            labels = labels.to(conf.device)\n-            batch_num += 1          \n-\n-            self.optimizer.zero_grad()\n-\n-            embeddings = self.model(imgs)\n-            thetas = self.head(embeddings, labels)\n-            loss = conf.ce_loss(thetas, labels)          \n-    \n-            #Compute the smoothed loss\n-            avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n-            self.writer.add_scalar(\'avg_loss\', avg_loss, batch_num)\n-            smoothed_loss = avg_loss / (1 - beta**batch_num)\n-            self.writer.add_scalar(\'smoothed_loss\', smoothed_loss,batch_num)\n-            #Stop if the loss is exploding\n-            if batch_num > 1 and smoothed_loss > bloding_scale * best_loss:\n-                print(\'exited with best_loss at {}\'.format(best_loss))\n-                plt.plot(log_lrs[10:-5], losses[10:-5])\n-                return log_lrs, losses\n-            #Record the best loss\n-            if smoothed_loss < best_loss or batch_num == 1:\n-                best_loss = smoothed_loss\n-            #Store the values\n-            losses.append(smoothed_loss)\n-            log_lrs.append(math.log10(lr))\n-            self.writer.add_scalar(\'log_lr\', math.log10(lr), batch_num)\n-            #Do the SGD step\n-            #Update the lr for the next step\n-\n-            loss.backward()\n-            self.optimizer.step()\n-\n-            lr *= mult\n-            for params in self.optimizer.param_groups:\n-                params[\'lr\'] = lr\n-            if batch_num > num:\n-                plt.plot(log_lrs[10:-5], losses[10:-5])\n-                return log_lrs, losses    \n-\n-    def train(self, conf, epochs):\n-        self.model.train()\n-        running_loss = 0.            \n-        for e in range(epochs):\n-            print(\'epoch {} started\'.format(e))\n-            if e == self.milestones[0]:\n-                self.schedule_lr()\n-            if e == self.milestones[1]:\n-                self.schedule_lr()      \n-            if e == self.milestones[2]:\n-                self.schedule_lr()                                 \n-            for imgs, labels in tqdm(iter(self.loader)):\n-                imgs = imgs.to(conf.device)\n-                labels = labels.to(conf.device)\n-                self.optimizer.zero_grad()\n-                embeddings = self.model(imgs)\n-                thetas = self.head(embeddings, labels)\n-                loss = conf.ce_loss(thetas, labels)\n-                loss.backward()\n-                running_loss += loss.item()\n-                self.optimizer.step()\n-                \n-                if self.step % self.board_loss_every == 0 and self.step != 0:\n-                    loss_board = running_loss / self.board_loss_every\n-                    self.writer.add_scalar(\'train_loss\', loss_board, self.step)\n-                    running_loss = 0.\n-                \n-                if self.step % self.evaluate_every == 0 and self.step != 0:\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.agedb_30, self.agedb_30_issame)\n-                    self.board_val(\'agedb_30\', accuracy, best_threshold, roc_curve_tensor)\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.lfw, self.lfw_issame)\n-                    self.board_val(\'lfw\', accuracy, best_threshold, roc_curve_tensor)\n-                    accuracy, best_threshold, roc_curve_tensor = self.evaluate(conf, self.cfp_fp, self.cfp_fp_issame)\n-                    self.board_val(\'cfp_fp\', accuracy, best_threshold, roc_curve_tensor)\n-                    self.model.train()\n-                if self.step % self.save_every == 0 and self.step != 0:\n-                    self.save_state(conf, accuracy)\n-                    \n-                self.step += 1\n-                \n-        self.save_state(conf, accuracy, to_save_folder=True, extra=\'final\')\n-\n-    def schedule_lr(self):\n-        for params in self.optimizer.param_groups:                 \n-            params[\'lr\'] /= 10\n-        print(self.optimizer)\n-    \n-    def infer(self, conf, faces, target_embs, tta=False):\n-        \'\'\'\n-        faces : list of PIL Image\n-        target_embs : [n, 512] computed embeddings of faces in facebank\n-        names : recorded names of faces in facebank\n-        tta : test time augmentation (hfilp, that\'s all)\n-        \'\'\'\n-        embs = []\n-        for img in faces:\n-            if tta:\n-                mirror = trans.functional.hflip(img)\n-                emb = self.model(conf.test_transform(img).to(conf.device).unsqueeze(0))\n-                emb_mirror = self.model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\n-                embs.append(l2_norm(emb + emb_mirror))\n-            else:                        \n-                embs.append(self.model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\n-        source_embs = torch.cat(embs)\n-        \n-        diff = source_embs.unsqueeze(-1) - target_embs.transpose(1,0).unsqueeze(0)\n-        dist = torch.sum(torch.pow(diff, 2), dim=1)\n-        minimum, min_idx = torch.min(dist, dim=1)\n-        min_idx[minimum > self.threshold] = -1 # if no match, set idx to -1\n-        return min_idx, minimum               \n\\ No newline at end of file\ndiff --git a/README.md b/README.md\ndeleted file mode 100644\nindex 624ca00..0000000\n--- a/README.md\n+++ /dev/null\n@@ -1,170 +0,0 @@\n-# Face Recognition using ARCFACE-Pytorch\n-\n-## Introduction\n- This repo contains face_verify.py and app.py which is able to perform the following task -\n- - Detect faces from an image, video or in webcam and perform face recogntion.\n- - app.py was used to deploy the project.\n- \n-## Required Files\n-- requirements.txt\n-- pretrained model [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate) or [Mobilefacenet @ OneDrive](https://onedrive.live.com/?authkey=%21AIweh1IfiuF9vm4&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21836&parId=root&o=OneUp).\n-- Custom dataset\n-- Newly Trained model (facebank.pth and names.npy)\n-\n-\n-## User Instruction\n-After downloading the project first you have to install the following libraries.\n-### Installation\n-You can install all the dependencies at once by running the following command from your terminal.\n-``` python\n-    $ pip install -r requirements.txt\n-```\n-##### For the installation of torch using "pip" run the following command\n-\n-``` python\n-    $ pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n-```\n-### Project Setup\n-\n-#### pre-trained model\n-Although i provided the pretrained model in the <b> work_space/model </b> and <b> work_space/save </b> folder, if you want to download the models you can follow the following url:\n-\n-- [IR-SE50 @ BaiduNetdisk](https://pan.baidu.com/s/12BUjjwy1uUTEF9HCx5qvoQ)\n-- [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate)\n-- [Mobilefacenet @ BaiduNetDisk](https://pan.baidu.com/s/1hqNNkcAjQOSxUjofboN6qg)\n-- [Mobilefacenet @ OneDrive](https://onedrive.live.com/?authkey=%21AIweh1IfiuF9vm4&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21836&parId=root&o=OneUp)\n-\n-I have used the <b>IR-SE50</b> as the pretrained model to train with my custom dataset. You need to copy the pretrained model and save it under the <b> work_space/save </b> folder as <b> model_final.pth</b>\n-\n-#### Newly trained model\n-In the <b> data/facebank </b> you will find a trained model named <b> "facebank.pth" </b> which contains the related weights and "names.npy" contains the corresponding labels of the users that are avialable in the facebank folder. For instance in this case\n-the <b> facebank </b> folder will look like this :-\n-\n-    facebank/\n-                ---> Chandler\n-                ---> Joey\n-                ---> Monica\n-                ---> Phoebe\n-                ---> Pias\n-                ---> Rachel\n-                ---> Raihan\n-                ---> Ross\n-                ---> Samiur\n-                ---> Shakil\n-                ---> facebank.pth\n-                ---> names.npy\n-\n-If you have the "facebank.pth" and "names.npy" files in the <b>data/facebank</b> you can execute the following command to see the demo.\n-\n-```python\n-    $ python app.py\n- ```\n- and go to the following url from your web browser.\n- ```url\n-http://localhost:5000\n-```\n-\n-\n-<hr>\n-Note: If you want to run the inference on a video, download a video of related persons (Person that you trained the model with) and replace 0 in the line number 43 of <b> face_verify.py </b> with the path of your video file. For this code you can run the inference on any video of <b> Friends</b> tv series.\n-<hr>\n-\n-#### Now if you want to train with your custom dataset, you need to follow the following steps.\n-\n-#### Dataset preparation \n-\n-First organize your images within the following manner- \n-\n-    data/\n-        raw/\n-             name1/\n-                 photo1.jpg\n-                 photo2.jpg\n-                 ...\n-             name2/\n-                 photo1.jpg\n-                 photo2.jpg\n-                 ...\n-             .....\n-now run the following command\n-```python\n-$ python .\\create-dataset\\align_dataset_mtcnn.py data/raw/ data/processed --image_size 112\n-```\n-\n-You will see a new folder inside the data directory named <b> "processed" </b> which will hold all the images that contains only faces of each user. If more than 1 image appears in any folder for a person, average embedding will be calculated. \n-\n-After executing the script new images for each user in the processed folder will look something like this.\n-<p align="center"> \n-<b> Cropped Images of faces </b>\n-    <img src ="http://muizzer07.pythonanywhere.com/media/files/Picture1.png">\n-</p> \n-\n-Copy all the folders of the users under the <b>data/processed</b> folder and paste in the <b>data/facebank</b> folder.\n-\n-\n-Now to train with your dataset, you need to set <b> args.update == True </b> in line 35 of face_verify.py . After training you will get a new facebank.pth and names.npy in your data/facebank folder which will now only holds the weights and labels of your newly trained dataset. Once the training is done you need to reset <b> args.update==False</b>.\n-However, if this doesn\'t work change the code in following manner-\n-#### Old Code \n-```python\n-    if args.update:\n-        targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\n-        print(\'facebank updated\')\n-    else:\n-        targets, names = load_facebank(conf)\n-        print(\'facebank loaded\')\n-```\n-#### New Code \n-Only keep the follwing lines for training, once the training is done just replace it with the old code.\n-```python\n-        targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\n-        print(\'facebank updated\')\n-````\n-Or you can simply pass a command line arguement such as below if there is new data to train.\n-```python\n-   $python face_verify.py -u\n-```\n-Here the -u parse the command to update the facebank.pth and names.npy.\n-\n-Now you are ready to test the systen with your newly trained users by running-\n-\n-```python\n-    $ python app.py\n-```\n-\n-### Note: You can train with custom dataset as many time as you want, you will only require any of the pre-trained model to train with your custom dataset to generate the <b>facebank.pth</b> and <b>names.npy</b>. Once you get this two files you are ready to test the face recogniton.\n-\n-\n-<hr>\n-\n-### Retrain the pre-trained model\n-\n- If you want to build a new pre-trained model like [IR-SE50 @ Onedrive](https://onedrive.live.com/?authkey=%21AOw5TZL8cWlj10I&cid=CEC0E1F8F0542A13&id=CEC0E1F8F0542A13%21835&parId=root&action=locate) and reproduce the result, you will need the large files which contains several dataset of faces under the <b> data/faces_emore </b>.\n- \n- To get these files, first you need to download the [MS1M](https://arxiv.org/abs/1607.08221) dataset from any of the following url-\n-- [emore dataset @ Dropbox](https://www.dropbox.com/s/wpx6tqjf0y5mf6r/faces_ms1m-refine-v2_112x112.zip?dl=0)\n-- [emore dataset @ BaiduDrive](https://pan.baidu.com/s/1eXohwNBHbbKXh5KHyItVhQ)\n-\n-After unzipping the downloaded file execute the following command. It will take few hours depending on your system configuration.\n-\n-```python\n-    $ python prepare_data.py\n-```\n-After that you will see the following files to the "data/faces_emore" folder. \n-\n-    faces_emore/\n-                ---> agedb_30\n-                ---> calfw\n-                ---> cfp_ff\n-                ---> cfp_fp\n-                ---> cfp_fp\n-                ---> cplfw\n-                ---> imgs\n-                ---> lfw\n-                ---> vgg2_fp\n-\n-To know the further training procedure you can see the details in this [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch) repository.\n-\n-## References\n-- [Arcface](https://arxiv.org/pdf/1801.07698.pdf)\n-- [InsightFace_Pytorch](https://github.com/TreB1eN/InsightFace_Pytorch)\n-- [The one with Face Recognition.](https://towardsdatascience.com/s01e01-3eb397d458d)\ndiff --git a/app.py b/app.py\ndeleted file mode 100644\nindex c84f09b..0000000\n--- a/app.py\n+++ /dev/null\n@@ -1,31 +0,0 @@\n-# Flask utils\n-from flask import Flask, redirect, url_for, request, render_template, Response\n-from werkzeug.utils import secure_filename\n-from gevent.pywsgi import WSGIServer\n-from face_verify import faceRec\n-\n-camera = faceRec()\n-\n-app = Flask(__name__)\n-\n-@app.route("/")\n-def main():\n-    return render_template("index.html")\n-\n-def gen(camera):\n-    while True:\n-        frame = camera.main()\n-        if frame != "":\n-            global_frame = frame\n-            yield (b\'--frame\\r\\n\'\n-                    b\'Content-Type: image/jpeg\\r\\n\\r\\n\' + frame + b\'\\r\\n\\r\\n\')\n-\n-@app.route(\'/video_feed\')\n-def video_feed():\n-    return Response(gen(camera), mimetype=\'multipart/x-mixed-replace; boundary=frame\')\n-\n-\n-if __name__ == \'__main__\':\n-    # Serve the app with gevent\n-    http_server = WSGIServer((\'localhost\', 5000), app)\n-    http_server.serve_forever()\n\\ No newline at end of file\ndiff --git a/config.py b/config.py\ndeleted file mode 100644\nindex 011c453..0000000\n--- a/config.py\n+++ /dev/null\n@@ -1,52 +0,0 @@\n-from easydict import EasyDict as edict\n-from pathlib import Path\n-import torch\n-from torch.nn import CrossEntropyLoss\n-from torchvision import transforms as trans\n-\n-def get_config(training = True):\n-    conf = edict()\n-    conf.data_path = Path(\'data\')\n-    conf.work_path = Path(\'work_space/\')\n-    conf.model_path = conf.work_path/\'models\'\n-    conf.log_path = conf.work_path/\'log\'\n-    conf.save_path = conf.work_path/\'save\'\n-    conf.input_size = [112,112]\n-    conf.embedding_size = 512\n-    conf.use_mobilfacenet = False\n-    conf.facebank_path = conf.data_path/\'facebank\'\n-    conf.net_depth = 50\n-    conf.drop_ratio = 0.6\n-    conf.net_mode = \'ir_se\' # or \'ir\'\n-    conf.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\n-    conf.test_transform = trans.Compose([\n-                    trans.ToTensor(),\n-                    trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n-                ])\n-    conf.data_mode = \'emore\'\n-    conf.vgg_folder = conf.data_path/\'faces_vgg_112x112\'\n-    conf.ms1m_folder = conf.data_path/\'faces_ms1m_112x112\'\n-    conf.emore_folder = conf.data_path/\'faces_emore\'\n-    conf.batch_size = 100 # irse net depth 50 \n-#   conf.batch_size = 200 # mobilefacenet\n-#--------------------Training Config ------------------------    \n-    if training:        \n-        conf.log_path = conf.work_path/\'log\'\n-        conf.save_path = conf.work_path/\'save\'\n-    #     conf.weight_decay = 5e-4\n-        conf.lr = 1e-3\n-        conf.milestones = [12,15,18]\n-        conf.momentum = 0.9\n-        conf.pin_memory = True\n-#         conf.num_workers = 4 # when batchsize is 200\n-        conf.num_workers = 3\n-        conf.ce_loss = CrossEntropyLoss()    \n-#--------------------Inference Config ------------------------\n-    else:\n-        conf.facebank_path = conf.data_path/\'facebank\'\n-        conf.threshold = 1.5\n-        conf.face_limit = 10 \n-        #when inference, at maximum detect 10 faces in one image,\n-        conf.min_face_size = 35\n-        # the larger this value, the faster deduction, comes with tradeoff in small faces\n-    return conf\ndiff --git a/create-dataset/align/__init__.py b/create-dataset/align/__init__.py\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/create-dataset/align/align_dataset_mtcnn.py b/create-dataset/align/align_dataset_mtcnn.py\ndeleted file mode 100644\nindex 7d5e735..0000000\n--- a/create-dataset/align/align_dataset_mtcnn.py\n+++ /dev/null\n@@ -1,159 +0,0 @@\n-"""Performs face alignment and stores face thumbnails in the output directory."""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import sys\n-import os\n-import argparse\n-import tensorflow as tf\n-import numpy as np\n-import facenet\n-import align.detect_face\n-import random\n-from time import sleep\n-\n-def main(args):\n-    sleep(random.random())\n-    output_dir = os.path.expanduser(args.output_dir)\n-    if not os.path.exists(output_dir):\n-        os.makedirs(output_dir)\n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n-    dataset = facenet.get_dataset(args.input_dir)\n-    \n-    print(\'Creating networks and loading parameters\')\n-    \n-    with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        with sess.as_default():\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n-    \n-    minsize = 20 # minimum size of face\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n-    factor = 0.709 # scale factor\n-\n-    # Add a random key to the filename to allow alignment using multiple processes\n-    random_key = np.random.randint(0, high=99999)\n-    bounding_boxes_filename = os.path.join(output_dir, \'bounding_boxes_%05d.txt\' % random_key)\n-    \n-    with open(bounding_boxes_filename, "w") as text_file:\n-        nrof_images_total = 0\n-        nrof_successfully_aligned = 0\n-        if args.random_order:\n-            random.shuffle(dataset)\n-        for cls in dataset:\n-            output_class_dir = os.path.join(output_dir, cls.name)\n-            if not os.path.exists(output_class_dir):\n-                os.makedirs(output_class_dir)\n-                if args.random_order:\n-                    random.shuffle(cls.image_paths)\n-            for image_path in cls.image_paths:\n-                nrof_images_total += 1\n-                filename = os.path.splitext(os.path.split(image_path)[1])[0]\n-                output_filename = os.path.join(output_class_dir, filename+\'.png\')\n-                print(image_path)\n-                if not os.path.exists(output_filename):\n-                    try:\n-                        img = misc.imread(image_path)\n-                    except (IOError, ValueError, IndexError) as e:\n-                        errorMessage = \'{}: {}\'.format(image_path, e)\n-                        print(errorMessage)\n-                    else:\n-                        if img.ndim<2:\n-                            print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n-                            continue\n-                        if img.ndim == 2:\n-                            img = facenet.to_rgb(img)\n-                        img = img[:,:,0:3]\n-    \n-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n-                        nrof_faces = bounding_boxes.shape[0]\n-                        if nrof_faces>0:\n-                            det = bounding_boxes[:,0:4]\n-                            det_arr = []\n-                            img_size = np.asarray(img.shape)[0:2]\n-                            if nrof_faces>1:\n-                                if args.detect_multiple_faces:\n-                                    for i in range(nrof_faces):\n-                                        det_arr.append(np.squeeze(det[i]))\n-                                else:\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n-                                    img_center = img_size / 2\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n-                                    det_arr.append(det[index,:])\n-                            else:\n-                                det_arr.append(np.squeeze(det))\n-\n-                            for i, det in enumerate(det_arr):\n-                                det = np.squeeze(det)\n-                                bb = np.zeros(4, dtype=np.int32)\n-                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\n-                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\n-                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\n-                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n-                                nrof_successfully_aligned += 1\n-                                filename_base, file_extension = os.path.splitext(output_filename)\n-                                if args.detect_multiple_faces:\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\n-                                else:\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\n-                                misc.imsave(output_filename_n, scaled)\n-                                text_file.write(\'%s %d %d %d %d\\n\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\n-                        else:\n-                            print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n-                            \n-    print(\'Total number of images: %d\' % nrof_images_total)\n-    print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n-            \n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'input_dir\', type=str, help=\'Directory with unaligned images.\')\n-    parser.add_argument(\'output_dir\', type=str, help=\'Directory with aligned face thumbnails.\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=182)\n-    parser.add_argument(\'--margin\', type=int,\n-        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n-    parser.add_argument(\'--random_order\', \n-        help=\'Shuffles the order of images to enable alignment using multiple processes.\', action=\'store_true\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--detect_multiple_faces\', type=bool,\n-                        help=\'Detect and align multiple faces per image.\', default=False)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/create-dataset/align/det1.npy b/create-dataset/align/det1.npy\ndeleted file mode 100644\nindex 7c05a2c..0000000\nBinary files a/create-dataset/align/det1.npy and /dev/null differ\ndiff --git a/create-dataset/align/det2.npy b/create-dataset/align/det2.npy\ndeleted file mode 100644\nindex 85d5bf0..0000000\nBinary files a/create-dataset/align/det2.npy and /dev/null differ\ndiff --git a/create-dataset/align/det3.npy b/create-dataset/align/det3.npy\ndeleted file mode 100644\nindex 90d5ba9..0000000\nBinary files a/create-dataset/align/det3.npy and /dev/null differ\ndiff --git a/create-dataset/align/detect_face.py b/create-dataset/align/detect_face.py\ndeleted file mode 100644\nindex 7f98ca7..0000000\n--- a/create-dataset/align/detect_face.py\n+++ /dev/null\n@@ -1,781 +0,0 @@\n-""" Tensorflow implementation of the face detection / alignment algorithm found at\n-https://github.com/kpzhang93/MTCNN_face_detection_alignment\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-from six import string_types, iteritems\n-\n-import numpy as np\n-import tensorflow as tf\n-#from math import floor\n-import cv2\n-import os\n-\n-def layer(op):\n-    """Decorator for composable network layers."""\n-\n-    def layer_decorated(self, *args, **kwargs):\n-        # Automatically set a name if not provided.\n-        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n-        # Figure out the layer inputs.\n-        if len(self.terminals) == 0:\n-            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n-        elif len(self.terminals) == 1:\n-            layer_input = self.terminals[0]\n-        else:\n-            layer_input = list(self.terminals)\n-        # Perform the operation and get the output.\n-        layer_output = op(self, layer_input, *args, **kwargs)\n-        # Add to layer LUT.\n-        self.layers[name] = layer_output\n-        # This output is now the input for the next layer.\n-        self.feed(layer_output)\n-        # Return self for chained calls.\n-        return self\n-\n-    return layer_decorated\n-\n-class Network(object):\n-\n-    def __init__(self, inputs, trainable=True):\n-        # The input nodes for this network\n-        self.inputs = inputs\n-        # The current list of terminal nodes\n-        self.terminals = []\n-        # Mapping from layer names to layers\n-        self.layers = dict(inputs)\n-        # If true, the resulting variables are set as trainable\n-        self.trainable = trainable\n-\n-        self.setup()\n-\n-    def setup(self):\n-        """Construct the network. """\n-        raise NotImplementedError(\'Must be implemented by the subclass.\')\n-\n-    def load(self, data_path, session, ignore_missing=False):\n-        """Load network weights.\n-        data_path: The path to the numpy-serialized network weights\n-        session: The current TensorFlow session\n-        ignore_missing: If true, serialized weights for missing layers are ignored.\n-        """\n-        data_dict = np.load(data_path, encoding=\'latin1\').item() #pylint: disable=no-member\n-\n-        for op_name in data_dict:\n-            with tf.variable_scope(op_name, reuse=True):\n-                for param_name, data in iteritems(data_dict[op_name]):\n-                    try:\n-                        var = tf.get_variable(param_name)\n-                        session.run(var.assign(data))\n-                    except ValueError:\n-                        if not ignore_missing:\n-                            raise\n-\n-    def feed(self, *args):\n-        """Set the input(s) for the next operation by replacing the terminal nodes.\n-        The arguments can be either layer names or the actual layers.\n-        """\n-        assert len(args) != 0\n-        self.terminals = []\n-        for fed_layer in args:\n-            if isinstance(fed_layer, string_types):\n-                try:\n-                    fed_layer = self.layers[fed_layer]\n-                except KeyError:\n-                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n-            self.terminals.append(fed_layer)\n-        return self\n-\n-    def get_output(self):\n-        """Returns the current network output."""\n-        return self.terminals[-1]\n-\n-    def get_unique_name(self, prefix):\n-        """Returns an index-suffixed unique name for the given prefix.\n-        This is used for auto-generating layer names based on the type-prefix.\n-        """\n-        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n-        return \'%s_%d\' % (prefix, ident)\n-\n-    def make_var(self, name, shape):\n-        """Creates a new TensorFlow variable."""\n-        return tf.get_variable(name, shape, trainable=self.trainable)\n-\n-    def validate_padding(self, padding):\n-        """Verifies that the padding is one of the supported ones."""\n-        assert padding in (\'SAME\', \'VALID\')\n-\n-    @layer\n-    def conv(self,\n-             inp,\n-             k_h,\n-             k_w,\n-             c_o,\n-             s_h,\n-             s_w,\n-             name,\n-             relu=True,\n-             padding=\'SAME\',\n-             group=1,\n-             biased=True):\n-        # Verify that the padding is acceptable\n-        self.validate_padding(padding)\n-        # Get the number of channels in the input\n-        c_i = int(inp.get_shape()[-1])\n-        # Verify that the grouping parameter is valid\n-        assert c_i % group == 0\n-        assert c_o % group == 0\n-        # Convolution for a given input and kernel\n-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n-        with tf.variable_scope(name) as scope:\n-            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n-            # This is the common-case. Convolve the input without any further complications.\n-            output = convolve(inp, kernel)\n-            # Add the biases\n-            if biased:\n-                biases = self.make_var(\'biases\', [c_o])\n-                output = tf.nn.bias_add(output, biases)\n-            if relu:\n-                # ReLU non-linearity\n-                output = tf.nn.relu(output, name=scope.name)\n-            return output\n-\n-    @layer\n-    def prelu(self, inp, name):\n-        with tf.variable_scope(name):\n-            i = int(inp.get_shape()[-1])\n-            alpha = self.make_var(\'alpha\', shape=(i,))\n-            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n-        return output\n-\n-    @layer\n-    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n-        self.validate_padding(padding)\n-        return tf.nn.max_pool(inp,\n-                              ksize=[1, k_h, k_w, 1],\n-                              strides=[1, s_h, s_w, 1],\n-                              padding=padding,\n-                              name=name)\n-\n-    @layer\n-    def fc(self, inp, num_out, name, relu=True):\n-        with tf.variable_scope(name):\n-            input_shape = inp.get_shape()\n-            if input_shape.ndims == 4:\n-                # The input is spatial. Vectorize it first.\n-                dim = 1\n-                for d in input_shape[1:].as_list():\n-                    dim *= int(d)\n-                feed_in = tf.reshape(inp, [-1, dim])\n-            else:\n-                feed_in, dim = (inp, input_shape[-1].value)\n-            weights = self.make_var(\'weights\', shape=[dim, num_out])\n-            biases = self.make_var(\'biases\', [num_out])\n-            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n-            fc = op(feed_in, weights, biases, name=name)\n-            return fc\n-\n-\n-    """\n-    Multi dimensional softmax,\n-    refer to https://github.com/tensorflow/tensorflow/issues/210\n-    compute softmax along the dimension of target\n-    the native softmax only supports batch_size x dimension\n-    """\n-    @layer\n-    def softmax(self, target, axis, name=None):\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\n-        target_exp = tf.exp(target-max_axis)\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n-        softmax = tf.div(target_exp, normalize, name)\n-        return softmax\n-    \n-class PNet(Network):\n-    def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 10, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'PReLU1\')\n-             .max_pool(2, 2, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 16, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'PReLU2\')\n-             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'PReLU3\')\n-             .conv(1, 1, 2, 1, 1, relu=False, name=\'conv4-1\')\n-             .softmax(3,name=\'prob1\'))\n-\n-        (self.feed(\'PReLU3\') #pylint: disable=no-value-for-parameter\n-             .conv(1, 1, 4, 1, 1, relu=False, name=\'conv4-2\'))\n-        \n-class RNet(Network):\n-    def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 28, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'prelu1\')\n-             .max_pool(3, 3, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 48, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'prelu2\')\n-             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n-             .conv(2, 2, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'prelu3\')\n-             .fc(128, relu=False, name=\'conv4\')\n-             .prelu(name=\'prelu4\')\n-             .fc(2, relu=False, name=\'conv5-1\')\n-             .softmax(1,name=\'prob1\'))\n-\n-        (self.feed(\'prelu4\') #pylint: disable=no-value-for-parameter\n-             .fc(4, relu=False, name=\'conv5-2\'))\n-\n-class ONet(Network):\n-    def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'prelu1\')\n-             .max_pool(3, 3, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'prelu2\')\n-             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n-             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'prelu3\')\n-             .max_pool(2, 2, 2, 2, name=\'pool3\')\n-             .conv(2, 2, 128, 1, 1, padding=\'VALID\', relu=False, name=\'conv4\')\n-             .prelu(name=\'prelu4\')\n-             .fc(256, relu=False, name=\'conv5\')\n-             .prelu(name=\'prelu5\')\n-             .fc(2, relu=False, name=\'conv6-1\')\n-             .softmax(1, name=\'prob1\'))\n-\n-        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n-             .fc(4, relu=False, name=\'conv6-2\'))\n-\n-        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n-             .fc(10, relu=False, name=\'conv6-3\'))\n-\n-def create_mtcnn(sess, model_path):\n-    if not model_path:\n-        model_path,_ = os.path.split(os.path.realpath(__file__))\n-\n-    with tf.variable_scope(\'pnet\'):\n-        data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n-        pnet = PNet({\'data\':data})\n-        pnet.load(os.path.join(model_path, \'det1.npy\'), sess)\n-    with tf.variable_scope(\'rnet\'):\n-        data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n-        rnet = RNet({\'data\':data})\n-        rnet.load(os.path.join(model_path, \'det2.npy\'), sess)\n-    with tf.variable_scope(\'onet\'):\n-        data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n-        onet = ONet({\'data\':data})\n-        onet.load(os.path.join(model_path, \'det3.npy\'), sess)\n-        \n-    pnet_fun = lambda img : sess.run((\'pnet/conv4-2/BiasAdd:0\', \'pnet/prob1:0\'), feed_dict={\'pnet/input:0\':img})\n-    rnet_fun = lambda img : sess.run((\'rnet/conv5-2/conv5-2:0\', \'rnet/prob1:0\'), feed_dict={\'rnet/input:0\':img})\n-    onet_fun = lambda img : sess.run((\'onet/conv6-2/conv6-2:0\', \'onet/conv6-3/conv6-3:0\', \'onet/prob1:0\'), feed_dict={\'onet/input:0\':img})\n-    return pnet_fun, rnet_fun, onet_fun\n-\n-def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n-    """Detects faces in an image, and returns bounding boxes and points for them.\n-    img: input image\n-    minsize: minimum faces\' size\n-    pnet, rnet, onet: caffemodel\n-    threshold: threshold=[th1, th2, th3], th1-3 are three steps\'s threshold\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n-    """\n-    factor_count=0\n-    total_boxes=np.empty((0,9))\n-    points=np.empty(0)\n-    h=img.shape[0]\n-    w=img.shape[1]\n-    minl=np.amin([h, w])\n-    m=12.0/minsize\n-    minl=minl*m\n-    # create scale pyramid\n-    scales=[]\n-    while minl>=12:\n-        scales += [m*np.power(factor, factor_count)]\n-        minl = minl*factor\n-        factor_count += 1\n-\n-    # first stage\n-    for scale in scales:\n-        hs=int(np.ceil(h*scale))\n-        ws=int(np.ceil(w*scale))\n-        im_data = imresample(img, (hs, ws))\n-        im_data = (im_data-127.5)*0.0078125\n-        img_x = np.expand_dims(im_data, 0)\n-        img_y = np.transpose(img_x, (0,2,1,3))\n-        out = pnet(img_y)\n-        out0 = np.transpose(out[0], (0,2,1,3))\n-        out1 = np.transpose(out[1], (0,2,1,3))\n-        \n-        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n-        \n-        # inter-scale nms\n-        pick = nms(boxes.copy(), 0.5, \'Union\')\n-        if boxes.size>0 and pick.size>0:\n-            boxes = boxes[pick,:]\n-            total_boxes = np.append(total_boxes, boxes, axis=0)\n-\n-    numbox = total_boxes.shape[0]\n-    if numbox>0:\n-        pick = nms(total_boxes.copy(), 0.7, \'Union\')\n-        total_boxes = total_boxes[pick,:]\n-        regw = total_boxes[:,2]-total_boxes[:,0]\n-        regh = total_boxes[:,3]-total_boxes[:,1]\n-        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\n-        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\n-        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\n-        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\n-        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\n-        total_boxes = rerec(total_boxes.copy())\n-        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n-\n-    numbox = total_boxes.shape[0]\n-    if numbox>0:\n-        # second stage\n-        tempimg = np.zeros((24,24,3,numbox))\n-        for k in range(0,numbox):\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n-                tempimg[:,:,:,k] = imresample(tmp, (24, 24))\n-            else:\n-                return np.empty()\n-        tempimg = (tempimg-127.5)*0.0078125\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n-        out = rnet(tempimg1)\n-        out0 = np.transpose(out[0])\n-        out1 = np.transpose(out[1])\n-        score = out1[1,:]\n-        ipass = np.where(score>threshold[1])\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n-        mv = out0[:,ipass[0]]\n-        if total_boxes.shape[0]>0:\n-            pick = nms(total_boxes, 0.7, \'Union\')\n-            total_boxes = total_boxes[pick,:]\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n-            total_boxes = rerec(total_boxes.copy())\n-\n-    numbox = total_boxes.shape[0]\n-    if numbox>0:\n-        # third stage\n-        total_boxes = np.fix(total_boxes).astype(np.int32)\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n-        tempimg = np.zeros((48,48,3,numbox))\n-        for k in range(0,numbox):\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n-                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\n-            else:\n-                return np.empty()\n-        tempimg = (tempimg-127.5)*0.0078125\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n-        out = onet(tempimg1)\n-        out0 = np.transpose(out[0])\n-        out1 = np.transpose(out[1])\n-        out2 = np.transpose(out[2])\n-        score = out2[1,:]\n-        points = out1\n-        ipass = np.where(score>threshold[2])\n-        points = points[:,ipass[0]]\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n-        mv = out0[:,ipass[0]]\n-\n-        w = total_boxes[:,2]-total_boxes[:,0]+1\n-        h = total_boxes[:,3]-total_boxes[:,1]+1\n-        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n-        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n-        if total_boxes.shape[0]>0:\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n-            pick = nms(total_boxes.copy(), 0.7, \'Min\')\n-            total_boxes = total_boxes[pick,:]\n-            points = points[:,pick]\n-                \n-    return total_boxes, points\n-\n-\n-def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\n-    """Detects faces in a list of images\n-    images: list containing input images\n-    detection_window_size_ratio: ratio of minimum face size to smallest image dimension\n-    pnet, rnet, onet: caffemodel\n-    threshold: threshold=[th1 th2 th3], th1-3 are three steps\'s threshold [0-1]\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n-    """\n-    all_scales = [None] * len(images)\n-    images_with_boxes = [None] * len(images)\n-\n-    for i in range(len(images)):\n-        images_with_boxes[i] = {\'total_boxes\': np.empty((0, 9))}\n-\n-    # create scale pyramid\n-    for index, img in enumerate(images):\n-        all_scales[index] = []\n-        h = img.shape[0]\n-        w = img.shape[1]\n-        minsize = int(detection_window_size_ratio * np.minimum(w, h))\n-        factor_count = 0\n-        minl = np.amin([h, w])\n-        if minsize <= 12:\n-            minsize = 12\n-\n-        m = 12.0 / minsize\n-        minl = minl * m\n-        while minl >= 12:\n-            all_scales[index].append(m * np.power(factor, factor_count))\n-            minl = minl * factor\n-            factor_count += 1\n-\n-    # # # # # # # # # # # # #\n-    # first stage - fast proposal network (pnet) to obtain face candidates\n-    # # # # # # # # # # # # #\n-\n-    images_obj_per_resolution = {}\n-\n-    # TODO: use some type of rounding to number module 8 to increase probability that pyramid images will have the same resolution across input images\n-\n-    for index, scales in enumerate(all_scales):\n-        h = images[index].shape[0]\n-        w = images[index].shape[1]\n-\n-        for scale in scales:\n-            hs = int(np.ceil(h * scale))\n-            ws = int(np.ceil(w * scale))\n-\n-            if (ws, hs) not in images_obj_per_resolution:\n-                images_obj_per_resolution[(ws, hs)] = []\n-\n-            im_data = imresample(images[index], (hs, ws))\n-            im_data = (im_data - 127.5) * 0.0078125\n-            img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\n-            images_obj_per_resolution[(ws, hs)].append({\'scale\': scale, \'image\': img_y, \'index\': index})\n-\n-    for resolution in images_obj_per_resolution:\n-        images_per_resolution = [i[\'image\'] for i in images_obj_per_resolution[resolution]]\n-        outs = pnet(images_per_resolution)\n-\n-        for index in range(len(outs[0])):\n-            scale = images_obj_per_resolution[resolution][index][\'scale\']\n-            image_index = images_obj_per_resolution[resolution][index][\'index\']\n-            out0 = np.transpose(outs[0][index], (1, 0, 2))\n-            out1 = np.transpose(outs[1][index], (1, 0, 2))\n-\n-            boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\n-\n-            # inter-scale nms\n-            pick = nms(boxes.copy(), 0.5, \'Union\')\n-            if boxes.size > 0 and pick.size > 0:\n-                boxes = boxes[pick, :]\n-                images_with_boxes[image_index][\'total_boxes\'] = np.append(images_with_boxes[image_index][\'total_boxes\'],\n-                                                                          boxes,\n-                                                                          axis=0)\n-\n-    for index, image_obj in enumerate(images_with_boxes):\n-        numbox = image_obj[\'total_boxes\'].shape[0]\n-        if numbox > 0:\n-            h = images[index].shape[0]\n-            w = images[index].shape[1]\n-            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Union\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            regw = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0]\n-            regh = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1]\n-            qq1 = image_obj[\'total_boxes\'][:, 0] + image_obj[\'total_boxes\'][:, 5] * regw\n-            qq2 = image_obj[\'total_boxes\'][:, 1] + image_obj[\'total_boxes\'][:, 6] * regh\n-            qq3 = image_obj[\'total_boxes\'][:, 2] + image_obj[\'total_boxes\'][:, 7] * regw\n-            qq4 = image_obj[\'total_boxes\'][:, 3] + image_obj[\'total_boxes\'][:, 8] * regh\n-            image_obj[\'total_boxes\'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj[\'total_boxes\'][:, 4]]))\n-            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n-            image_obj[\'total_boxes\'][:, 0:4] = np.fix(image_obj[\'total_boxes\'][:, 0:4]).astype(np.int32)\n-            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n-\n-            numbox = image_obj[\'total_boxes\'].shape[0]\n-            tempimg = np.zeros((24, 24, 3, numbox))\n-\n-            if numbox > 0:\n-                for k in range(0, numbox):\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n-                        tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n-                    else:\n-                        return np.empty()\n-\n-                tempimg = (tempimg - 127.5) * 0.0078125\n-                image_obj[\'rnet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n-\n-    # # # # # # # # # # # # #\n-    # second stage - refinement of face candidates with rnet\n-    # # # # # # # # # # # # #\n-\n-    bulk_rnet_input = np.empty((0, 24, 24, 3))\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'rnet_input\' in image_obj:\n-            bulk_rnet_input = np.append(bulk_rnet_input, image_obj[\'rnet_input\'], axis=0)\n-\n-    out = rnet(bulk_rnet_input)\n-    out0 = np.transpose(out[0])\n-    out1 = np.transpose(out[1])\n-    score = out1[1, :]\n-\n-    i = 0\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'rnet_input\' not in image_obj:\n-            continue\n-\n-        rnet_input_count = image_obj[\'rnet_input\'].shape[0]\n-        score_per_image = score[i:i + rnet_input_count]\n-        out0_per_image = out0[:, i:i + rnet_input_count]\n-\n-        ipass = np.where(score_per_image > threshold[1])\n-        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n-\n-        mv = out0_per_image[:, ipass[0]]\n-\n-        if image_obj[\'total_boxes\'].shape[0] > 0:\n-            h = images[index].shape[0]\n-            w = images[index].shape[1]\n-            pick = nms(image_obj[\'total_boxes\'], 0.7, \'Union\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv[:, pick]))\n-            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n-\n-            numbox = image_obj[\'total_boxes\'].shape[0]\n-\n-            if numbox > 0:\n-                tempimg = np.zeros((48, 48, 3, numbox))\n-                image_obj[\'total_boxes\'] = np.fix(image_obj[\'total_boxes\']).astype(np.int32)\n-                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n-\n-                for k in range(0, numbox):\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n-                        tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n-                    else:\n-                        return np.empty()\n-                tempimg = (tempimg - 127.5) * 0.0078125\n-                image_obj[\'onet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n-\n-        i += rnet_input_count\n-\n-    # # # # # # # # # # # # #\n-    # third stage - further refinement and facial landmarks positions with onet\n-    # # # # # # # # # # # # #\n-\n-    bulk_onet_input = np.empty((0, 48, 48, 3))\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'onet_input\' in image_obj:\n-            bulk_onet_input = np.append(bulk_onet_input, image_obj[\'onet_input\'], axis=0)\n-\n-    out = onet(bulk_onet_input)\n-\n-    out0 = np.transpose(out[0])\n-    out1 = np.transpose(out[1])\n-    out2 = np.transpose(out[2])\n-    score = out2[1, :]\n-    points = out1\n-\n-    i = 0\n-    ret = []\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'onet_input\' not in image_obj:\n-            ret.append(None)\n-            continue\n-\n-        onet_input_count = image_obj[\'onet_input\'].shape[0]\n-\n-        out0_per_image = out0[:, i:i + onet_input_count]\n-        score_per_image = score[i:i + onet_input_count]\n-        points_per_image = points[:, i:i + onet_input_count]\n-\n-        ipass = np.where(score_per_image > threshold[2])\n-        points_per_image = points_per_image[:, ipass[0]]\n-\n-        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n-        mv = out0_per_image[:, ipass[0]]\n-\n-        w = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0] + 1\n-        h = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1] + 1\n-        points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\n-            image_obj[\'total_boxes\'][:, 0], (5, 1)) - 1\n-        points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\n-            image_obj[\'total_boxes\'][:, 1], (5, 1)) - 1\n-\n-        if image_obj[\'total_boxes\'].shape[0] > 0:\n-            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv))\n-            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Min\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            points_per_image = points_per_image[:, pick]\n-\n-            ret.append((image_obj[\'total_boxes\'], points_per_image))\n-        else:\n-            ret.append(None)\n-\n-        i += onet_input_count\n-\n-    return ret\n-\n-\n-# function [boundingbox] = bbreg(boundingbox,reg)\n-def bbreg(boundingbox,reg):\n-    """Calibrate bounding boxes"""\n-    if reg.shape[1]==1:\n-        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n-\n-    w = boundingbox[:,2]-boundingbox[:,0]+1\n-    h = boundingbox[:,3]-boundingbox[:,1]+1\n-    b1 = boundingbox[:,0]+reg[:,0]*w\n-    b2 = boundingbox[:,1]+reg[:,1]*h\n-    b3 = boundingbox[:,2]+reg[:,2]*w\n-    b4 = boundingbox[:,3]+reg[:,3]*h\n-    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\n-    return boundingbox\n- \n-def generateBoundingBox(imap, reg, scale, t):\n-    """Use heatmap to generate bounding boxes"""\n-    stride=2\n-    cellsize=12\n-\n-    imap = np.transpose(imap)\n-    dx1 = np.transpose(reg[:,:,0])\n-    dy1 = np.transpose(reg[:,:,1])\n-    dx2 = np.transpose(reg[:,:,2])\n-    dy2 = np.transpose(reg[:,:,3])\n-    y, x = np.where(imap >= t)\n-    if y.shape[0]==1:\n-        dx1 = np.flipud(dx1)\n-        dy1 = np.flipud(dy1)\n-        dx2 = np.flipud(dx2)\n-        dy2 = np.flipud(dy2)\n-    score = imap[(y,x)]\n-    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\n-    if reg.size==0:\n-        reg = np.empty((0,3))\n-    bb = np.transpose(np.vstack([y,x]))\n-    q1 = np.fix((stride*bb+1)/scale)\n-    q2 = np.fix((stride*bb+cellsize-1+1)/scale)\n-    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\n-    return boundingbox, reg\n- \n-# function pick = nms(boxes,threshold,type)\n-def nms(boxes, threshold, method):\n-    if boxes.size==0:\n-        return np.empty((0,3))\n-    x1 = boxes[:,0]\n-    y1 = boxes[:,1]\n-    x2 = boxes[:,2]\n-    y2 = boxes[:,3]\n-    s = boxes[:,4]\n-    area = (x2-x1+1) * (y2-y1+1)\n-    I = np.argsort(s)\n-    pick = np.zeros_like(s, dtype=np.int16)\n-    counter = 0\n-    while I.size>0:\n-        i = I[-1]\n-        pick[counter] = i\n-        counter += 1\n-        idx = I[0:-1]\n-        xx1 = np.maximum(x1[i], x1[idx])\n-        yy1 = np.maximum(y1[i], y1[idx])\n-        xx2 = np.minimum(x2[i], x2[idx])\n-        yy2 = np.minimum(y2[i], y2[idx])\n-        w = np.maximum(0.0, xx2-xx1+1)\n-        h = np.maximum(0.0, yy2-yy1+1)\n-        inter = w * h\n-        if method is \'Min\':\n-            o = inter / np.minimum(area[i], area[idx])\n-        else:\n-            o = inter / (area[i] + area[idx] - inter)\n-        I = I[np.where(o<=threshold)]\n-    pick = pick[0:counter]\n-    return pick\n-\n-# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\n-def pad(total_boxes, w, h):\n-    """Compute the padding coordinates (pad the bounding boxes to square)"""\n-    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\n-    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\n-    numbox = total_boxes.shape[0]\n-\n-    dx = np.ones((numbox), dtype=np.int32)\n-    dy = np.ones((numbox), dtype=np.int32)\n-    edx = tmpw.copy().astype(np.int32)\n-    edy = tmph.copy().astype(np.int32)\n-\n-    x = total_boxes[:,0].copy().astype(np.int32)\n-    y = total_boxes[:,1].copy().astype(np.int32)\n-    ex = total_boxes[:,2].copy().astype(np.int32)\n-    ey = total_boxes[:,3].copy().astype(np.int32)\n-\n-    tmp = np.where(ex>w)\n-    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\n-    ex[tmp] = w\n-    \n-    tmp = np.where(ey>h)\n-    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\n-    ey[tmp] = h\n-\n-    tmp = np.where(x<1)\n-    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\n-    x[tmp] = 1\n-\n-    tmp = np.where(y<1)\n-    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\n-    y[tmp] = 1\n-    \n-    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n-\n-# function [bboxA] = rerec(bboxA)\n-def rerec(bboxA):\n-    """Convert bboxA to square."""\n-    h = bboxA[:,3]-bboxA[:,1]\n-    w = bboxA[:,2]-bboxA[:,0]\n-    l = np.maximum(w, h)\n-    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\n-    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\n-    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\n-    return bboxA\n-\n-def imresample(img, sz):\n-    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\n-    return im_data\n-\n-    # This method is kept for debugging purpose\n-#     h=img.shape[0]\n-#     w=img.shape[1]\n-#     hs, ws = sz\n-#     dx = float(w) / ws\n-#     dy = float(h) / hs\n-#     im_data = np.zeros((hs,ws,3))\n-#     for a1 in range(0,hs):\n-#         for a2 in range(0,ws):\n-#             for a3 in range(0,3):\n-#                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\n-#     return im_data\n-\ndiff --git a/create-dataset/align_dataset_mtcnn.py b/create-dataset/align_dataset_mtcnn.py\ndeleted file mode 100644\nindex 2d8110e..0000000\n--- a/create-dataset/align_dataset_mtcnn.py\n+++ /dev/null\n@@ -1,160 +0,0 @@\n-"""Performs face alignment and stores face thumbnails in the output directory."""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import sys\n-import os\n-import argparse\n-import tensorflow as tf\n-import numpy as np\n-import facenet\n-import align.detect_face\n-import random\n-import imageio\n-from time import sleep\n-\n-def main(args):\n-    sleep(random.random())\n-    output_dir = os.path.expanduser(args.output_dir)\n-    if not os.path.exists(output_dir):\n-        os.makedirs(output_dir)\n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n-    dataset = facenet.get_dataset(args.input_dir)\n-    \n-    print(\'Creating networks and loading parameters\')\n-    \n-    with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        with sess.as_default():\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n-    \n-    minsize = 20 # minimum size of face\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n-    factor = 0.709 # scale factor\n-\n-    # Add a random key to the filename to allow alignment using multiple processes\n-    random_key = np.random.randint(0, high=99999)\n-    bounding_boxes_filename = os.path.join(output_dir, \'bounding_boxes_%05d.txt\' % random_key)\n-    \n-    with open(bounding_boxes_filename, "w") as text_file:\n-        nrof_images_total = 0\n-        nrof_successfully_aligned = 0\n-        if args.random_order:\n-            random.shuffle(dataset)\n-        for cls in dataset:\n-            output_class_dir = os.path.join(output_dir, cls.name)\n-            if not os.path.exists(output_class_dir):\n-                os.makedirs(output_class_dir)\n-                if args.random_order:\n-                    random.shuffle(cls.image_paths)\n-            for image_path in cls.image_paths:\n-                nrof_images_total += 1\n-                filename = os.path.splitext(os.path.split(image_path)[1])[0]\n-                output_filename = os.path.join(output_class_dir, filename+\'.png\')\n-                print(image_path)\n-                if not os.path.exists(output_filename):\n-                    try:\n-                        img = imageio.imread(image_path)\n-                    except (IOError, ValueError, IndexError) as e:\n-                        errorMessage = \'{}: {}\'.format(image_path, e)\n-                        print(errorMessage)\n-                    else:\n-                        if img.ndim<2:\n-                            print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n-                            continue\n-                        if img.ndim == 2:\n-                            img = facenet.to_rgb(img)\n-                        img = img[:,:,0:3]\n-    \n-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n-                        nrof_faces = bounding_boxes.shape[0]\n-                        if nrof_faces>0:\n-                            det = bounding_boxes[:,0:4]\n-                            det_arr = []\n-                            img_size = np.asarray(img.shape)[0:2]\n-                            if nrof_faces>1:\n-                                if args.detect_multiple_faces:\n-                                    for i in range(nrof_faces):\n-                                        det_arr.append(np.squeeze(det[i]))\n-                                else:\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n-                                    img_center = img_size / 2\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n-                                    det_arr.append(det[index,:])\n-                            else:\n-                                det_arr.append(np.squeeze(det))\n-\n-                            for i, det in enumerate(det_arr):\n-                                det = np.squeeze(det)\n-                                bb = np.zeros(4, dtype=np.int32)\n-                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\n-                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\n-                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\n-                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n-                                nrof_successfully_aligned += 1\n-                                filename_base, file_extension = os.path.splitext(output_filename)\n-                                if args.detect_multiple_faces:\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\n-                                else:\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\n-                                misc.imsave(output_filename_n, scaled)\n-                                text_file.write(\'%s %d %d %d %d\\n\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\n-                        else:\n-                            print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n-                            \n-    print(\'Total number of images: %d\' % nrof_images_total)\n-    print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n-            \n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'input_dir\', type=str, help=\'Directory with unaligned images.\')\n-    parser.add_argument(\'output_dir\', type=str, help=\'Directory with aligned face thumbnails.\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=182)\n-    parser.add_argument(\'--margin\', type=int,\n-        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n-    parser.add_argument(\'--random_order\', \n-        help=\'Shuffles the order of images to enable alignment using multiple processes.\', action=\'store_true\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--detect_multiple_faces\', type=bool,\n-                        help=\'Detect and align multiple faces per image.\', default=False)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/create-dataset/facenet.py b/create-dataset/facenet.py\ndeleted file mode 100644\nindex 0e05676..0000000\n--- a/create-dataset/facenet.py\n+++ /dev/null\n@@ -1,571 +0,0 @@\n-"""Functions for building the face recognition network.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-# pylint: disable=missing-docstring\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import os\n-from subprocess import Popen, PIPE\n-import tensorflow as tf\n-import numpy as np\n-from scipy import misc\n-from sklearn.model_selection import KFold\n-from scipy import interpolate\n-from tensorflow.python.training import training\n-import random\n-import re\n-from tensorflow.python.platform import gfile\n-import math\n-from six import iteritems\n-\n-def triplet_loss(anchor, positive, negative, alpha):\n-    """Calculate the triplet loss according to the FaceNet paper\n-    \n-    Args:\n-      anchor: the embeddings for the anchor images.\n-      positive: the embeddings for the positive images.\n-      negative: the embeddings for the negative images.\n-  \n-    Returns:\n-      the triplet loss according to the FaceNet paper as a float tensor.\n-    """\n-    with tf.variable_scope(\'triplet_loss\'):\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n-        \n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n-      \n-    return loss\n-  \n-def center_loss(features, label, alfa, nrof_classes):\n-    """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n-       (http://ydwen.github.io/papers/WenECCV16.pdf)\n-    """\n-    nrof_features = features.get_shape()[1]\n-    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n-    label = tf.reshape(label, [-1])\n-    centers_batch = tf.gather(centers, label)\n-    diff = (1 - alfa) * (centers_batch - features)\n-    centers = tf.scatter_sub(centers, label, diff)\n-    with tf.control_dependencies([centers]):\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\n-    return loss, centers\n-\n-def get_image_paths_and_labels(dataset):\n-    image_paths_flat = []\n-    labels_flat = []\n-    for i in range(len(dataset)):\n-        image_paths_flat += dataset[i].image_paths\n-        labels_flat += [i] * len(dataset[i].image_paths)\n-    return image_paths_flat, labels_flat\n-\n-def shuffle_examples(image_paths, labels):\n-    shuffle_list = list(zip(image_paths, labels))\n-    random.shuffle(shuffle_list)\n-    image_paths_shuff, labels_shuff = zip(*shuffle_list)\n-    return image_paths_shuff, labels_shuff\n-\n-def random_rotate_image(image):\n-    angle = np.random.uniform(low=-10.0, high=10.0)\n-    return misc.imrotate(image, angle, \'bicubic\')\n-  \n-# 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n-RANDOM_ROTATE = 1\n-RANDOM_CROP = 2\n-RANDOM_FLIP = 4\n-FIXED_STANDARDIZATION = 8\n-FLIP = 16\n-def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n-    images_and_labels_list = []\n-    for _ in range(nrof_preprocess_threads):\n-        filenames, label, control = input_queue.dequeue()\n-        images = []\n-        for filename in tf.unstack(filenames):\n-            file_contents = tf.read_file(filename)\n-            image = tf.image.decode_image(file_contents, 3)\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n-            image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            #pylint: disable=no-member\n-            image.set_shape(image_size + (3,))\n-            images.append(image)\n-        images_and_labels_list.append([images, label])\n-\n-    image_batch, label_batch = tf.train.batch_join(\n-        images_and_labels_list, batch_size=batch_size_placeholder, \n-        shapes=[image_size + (3,), ()], enqueue_many=True,\n-        capacity=4 * nrof_preprocess_threads * 100,\n-        allow_smaller_final_batch=True)\n-    \n-    return image_batch, label_batch\n-\n-def get_control_flag(control, field):\n-    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n-  \n-def _add_loss_summaries(total_loss):\n-    """Add summaries for losses.\n-  \n-    Generates moving average for all losses and associated summaries for\n-    visualizing the performance of the network.\n-  \n-    Args:\n-      total_loss: Total loss from loss().\n-    Returns:\n-      loss_averages_op: op for generating moving averages of losses.\n-    """\n-    # Compute the moving average of all individual losses and the total loss.\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n-    losses = tf.get_collection(\'losses\')\n-    loss_averages_op = loss_averages.apply(losses + [total_loss])\n-  \n-    # Attach a scalar summmary to all individual losses and the total loss; do the\n-    # same for the averaged version of the losses.\n-    for l in losses + [total_loss]:\n-        # Name each loss as \'(raw)\' and name the moving average version of the loss\n-        # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\n-  \n-    return loss_averages_op\n-\n-def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n-    # Generate moving averages of all losses and associated summaries.\n-    loss_averages_op = _add_loss_summaries(total_loss)\n-\n-    # Compute gradients.\n-    with tf.control_dependencies([loss_averages_op]):\n-        if optimizer==\'ADAGRAD\':\n-            opt = tf.train.AdagradOptimizer(learning_rate)\n-        elif optimizer==\'ADADELTA\':\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n-        elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n-        elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n-        elif optimizer==\'MOM\':\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n-        else:\n-            raise ValueError(\'Invalid optimization algorithm\')\n-    \n-        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n-        \n-    # Apply gradients.\n-    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n-  \n-    # Add histograms for trainable variables.\n-    if log_histograms:\n-        for var in tf.trainable_variables():\n-            tf.summary.histogram(var.op.name, var)\n-   \n-    # Add histograms for gradients.\n-    if log_histograms:\n-        for grad, var in grads:\n-            if grad is not None:\n-                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n-  \n-    # Track the moving averages of all trainable variables.\n-    variable_averages = tf.train.ExponentialMovingAverage(\n-        moving_average_decay, global_step)\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n-  \n-    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n-        train_op = tf.no_op(name=\'train\')\n-  \n-    return train_op\n-\n-def prewhiten(x):\n-    mean = np.mean(x)\n-    std = np.std(x)\n-    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n-    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n-    return y  \n-\n-def crop(image, random_crop, image_size):\n-    if image.shape[1]>image_size:\n-        sz1 = int(image.shape[1]//2)\n-        sz2 = int(image_size//2)\n-        if random_crop:\n-            diff = sz1-sz2\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n-        else:\n-            (h, v) = (0,0)\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n-    return image\n-  \n-def flip(image, random_flip):\n-    if random_flip and np.random.choice([True, False]):\n-        image = np.fliplr(image)\n-    return image\n-\n-def to_rgb(img):\n-    w, h = img.shape\n-    ret = np.empty((w, h, 3), dtype=np.uint8)\n-    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n-    return ret\n-  \n-def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n-    nrof_samples = len(image_paths)\n-    images = np.zeros((nrof_samples, image_size, image_size, 3))\n-    for i in range(nrof_samples):\n-        img = misc.imread(image_paths[i])\n-        if img.ndim == 2:\n-            img = to_rgb(img)\n-        if do_prewhiten:\n-            img = prewhiten(img)\n-        img = crop(img, do_random_crop, image_size)\n-        img = flip(img, do_random_flip)\n-        images[i,:,:,:] = img\n-    return images\n-\n-def get_label_batch(label_data, batch_size, batch_index):\n-    nrof_examples = np.size(label_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = label_data[j:j+batch_size]\n-    else:\n-        x1 = label_data[j:nrof_examples]\n-        x2 = label_data[0:nrof_examples-j]\n-        batch = np.vstack([x1,x2])\n-    batch_int = batch.astype(np.int64)\n-    return batch_int\n-\n-def get_batch(image_data, batch_size, batch_index):\n-    nrof_examples = np.size(image_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = image_data[j:j+batch_size,:,:,:]\n-    else:\n-        x1 = image_data[j:nrof_examples,:,:,:]\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\n-        batch = np.vstack([x1,x2])\n-    batch_float = batch.astype(np.float32)\n-    return batch_float\n-\n-def get_triplet_batch(triplets, batch_index, batch_size):\n-    ax, px, nx = triplets\n-    a = get_batch(ax, int(batch_size/3), batch_index)\n-    p = get_batch(px, int(batch_size/3), batch_index)\n-    n = get_batch(nx, int(batch_size/3), batch_index)\n-    batch = np.vstack([a, p, n])\n-    return batch\n-\n-def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n-        for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n-            if line:\n-                par = line.strip().split(\':\')\n-                e = int(par[0])\n-                if par[1]==\'-\':\n-                    lr = -1\n-                else:\n-                    lr = float(par[1])\n-                if e <= epoch:\n-                    learning_rate = lr\n-                else:\n-                    return learning_rate\n-\n-class ImageClass():\n-    "Stores the paths to images for a given class"\n-    def __init__(self, name, image_paths):\n-        self.name = name\n-        self.image_paths = image_paths\n-  \n-    def __str__(self):\n-        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n-  \n-    def __len__(self):\n-        return len(self.image_paths)\n-  \n-def get_dataset(path, has_class_directories=True):\n-    dataset = []\n-    path_exp = os.path.expanduser(path)\n-    classes = [path for path in os.listdir(path_exp) \\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\n-    classes.sort()\n-    nrof_classes = len(classes)\n-    for i in range(nrof_classes):\n-        class_name = classes[i]\n-        facedir = os.path.join(path_exp, class_name)\n-        image_paths = get_image_paths(facedir)\n-        dataset.append(ImageClass(class_name, image_paths))\n-  \n-    return dataset\n-\n-def get_image_paths(facedir):\n-    image_paths = []\n-    if os.path.isdir(facedir):\n-        images = os.listdir(facedir)\n-        image_paths = [os.path.join(facedir,img) for img in images]\n-    return image_paths\n-  \n-def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n-    if mode==\'SPLIT_CLASSES\':\n-        nrof_classes = len(dataset)\n-        class_indices = np.arange(nrof_classes)\n-        np.random.shuffle(class_indices)\n-        split = int(round(nrof_classes*(1-split_ratio)))\n-        train_set = [dataset[i] for i in class_indices[0:split]]\n-        test_set = [dataset[i] for i in class_indices[split:-1]]\n-    elif mode==\'SPLIT_IMAGES\':\n-        train_set = []\n-        test_set = []\n-        for cls in dataset:\n-            paths = cls.image_paths\n-            np.random.shuffle(paths)\n-            nrof_images_in_class = len(paths)\n-            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n-            if split==nrof_images_in_class:\n-                split = nrof_images_in_class-1\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n-                train_set.append(ImageClass(cls.name, paths[:split]))\n-                test_set.append(ImageClass(cls.name, paths[split:]))\n-    else:\n-        raise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n-    return train_set, test_set\n-\n-def load_model(model, input_map=None):\n-    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n-    #  or if it is a protobuf file with a frozen graph\n-    model_exp = os.path.expanduser(model)\n-    if (os.path.isfile(model_exp)):\n-        print(\'Model filename: %s\' % model_exp)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n-            graph_def.ParseFromString(f.read())\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n-    else:\n-        print(\'Model directory: %s\' % model_exp)\n-        meta_file, ckpt_file = get_model_filenames(model_exp)\n-        \n-        print(\'Metagraph file: %s\' % meta_file)\n-        print(\'Checkpoint file: %s\' % ckpt_file)\n-      \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n-    \n-def get_model_filenames(model_dir):\n-    files = os.listdir(model_dir)\n-    meta_files = [s for s in files if s.endswith(\'.meta\')]\n-    if len(meta_files)==0:\n-        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n-    elif len(meta_files)>1:\n-        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n-    meta_file = meta_files[0]\n-    ckpt = tf.train.get_checkpoint_state(model_dir)\n-    if ckpt and ckpt.model_checkpoint_path:\n-        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n-        return meta_file, ckpt_file\n-\n-    meta_files = [s for s in files if \'.ckpt\' in s]\n-    max_step = -1\n-    for f in files:\n-        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n-        if step_str is not None and len(step_str.groups())>=2:\n-            step = int(step_str.groups()[1])\n-            if step > max_step:\n-                max_step = step\n-                ckpt_file = step_str.groups()[0]\n-    return meta_file, ckpt_file\n-  \n-def distance(embeddings1, embeddings2, distance_metric=0):\n-    if distance_metric==0:\n-        # Euclidian distance\n-        diff = np.subtract(embeddings1, embeddings2)\n-        dist = np.sum(np.square(diff),1)\n-    elif distance_metric==1:\n-        # Distance based on cosine similarity\n-        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n-        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n-        similarity = dot / norm\n-        dist = np.arccos(similarity) / math.pi\n-    else:\n-        raise \'Undefined distance metric %d\' % distance_metric \n-        \n-    return dist\n-\n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\n-    accuracy = np.zeros((nrof_folds))\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-        \n-        # Find the best threshold for the fold\n-        acc_train = np.zeros((nrof_thresholds))\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n-        best_threshold_index = np.argmax(acc_train)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n-          \n-        tpr = np.mean(tprs,0)\n-        fpr = np.mean(fprs,0)\n-    return tpr, fpr, accuracy\n-\n-def calculate_accuracy(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n-  \n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n-    acc = float(tp+tn)/dist.size\n-    return tpr, fpr, acc\n-\n-\n-  \n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    val = np.zeros(nrof_folds)\n-    far = np.zeros(nrof_folds)\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-      \n-        # Find the threshold that gives FAR = far_target\n-        far_train = np.zeros(nrof_thresholds)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n-        if np.max(far_train)>=far_target:\n-            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n-            threshold = f(far_target)\n-        else:\n-            threshold = 0.0\n-    \n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n-  \n-    val_mean = np.mean(val)\n-    far_mean = np.mean(far)\n-    val_std = np.std(val)\n-    return val_mean, val_std, far_mean\n-\n-\n-def calculate_val_far(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    n_same = np.sum(actual_issame)\n-    n_diff = np.sum(np.logical_not(actual_issame))\n-    val = float(true_accept) / float(n_same)\n-    far = float(false_accept) / float(n_diff)\n-    return val, far\n-\n-def store_revision_info(src_path, output_dir, arg_string):\n-    try:\n-        # Get git hash\n-        cmd = [\'git\', \'rev-parse\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_hash = stdout.strip()\n-    except OSError as e:\n-        git_hash = \' \'.join(cmd) + \': \' +  e.strerror\n-  \n-    try:\n-        # Get local changes\n-        cmd = [\'git\', \'diff\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_diff = stdout.strip()\n-    except OSError as e:\n-        git_diff = \' \'.join(cmd) + \': \' +  e.strerror\n-    \n-    # Store a text file in the log directory\n-    rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n-    with open(rev_info_filename, "w") as text_file:\n-        text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n-        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n-        text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n-        text_file.write(\'%s\' % git_diff)\n-\n-def list_variables(filename):\n-    reader = training.NewCheckpointReader(filename)\n-    variable_map = reader.get_variable_to_shape_map()\n-    names = sorted(variable_map.keys())\n-    return names\n-\n-def put_images_on_grid(images, shape=(16,8)):\n-    nrof_images = images.shape[0]\n-    img_size = images.shape[1]\n-    bw = 3\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n-    for i in range(shape[1]):\n-        x_start = i*(img_size+bw)+bw\n-        for j in range(shape[0]):\n-            img_index = i*shape[0]+j\n-            if img_index>=nrof_images:\n-                break\n-            y_start = j*(img_size+bw)+bw\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n-        if img_index>=nrof_images:\n-            break\n-    return img\n-\n-def write_arguments_to_file(args, filename):\n-    with open(filename, \'w\') as f:\n-        for key, value in iteritems(vars(args)):\n-            f.write(\'%s: %s\\n\' % (key, str(value)))\ndiff --git a/data/__init__.py b/data/__init__.py\ndeleted file mode 100644\nindex e69de29..0000000\ndiff --git a/data/data_pipe.py b/data/data_pipe.py\ndeleted file mode 100644\nindex bd67f02..0000000\n--- a/data/data_pipe.py\n+++ /dev/null\n@@ -1,122 +0,0 @@\n-from pathlib import Path\n-from torch.utils.data import Dataset, ConcatDataset, DataLoader\n-from torchvision import transforms as trans\n-from torchvision.datasets import ImageFolder\n-from PIL import Image, ImageFile\n-ImageFile.LOAD_TRUNCATED_IMAGES = True\n-import numpy as np\n-import cv2\n-import bcolz\n-import pickle\n-import torch\n-import mxnet as mx\n-from tqdm import tqdm\n-\n-def de_preprocess(tensor):\n-    return tensor*0.5 + 0.5\n-    \n-def get_train_dataset(imgs_folder):\n-    train_transform = trans.Compose([\n-        trans.RandomHorizontalFlip(),\n-        trans.ToTensor(),\n-        trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n-    ])\n-    ds = ImageFolder(imgs_folder, train_transform)\n-    class_num = ds[-1][1] + 1\n-    return ds, class_num\n-\n-def get_train_loader(conf):\n-    if conf.data_mode in [\'ms1m\', \'concat\']:\n-        ms1m_ds, ms1m_class_num = get_train_dataset(conf.ms1m_folder/\'imgs\')\n-        print(\'ms1m loader generated\')\n-    if conf.data_mode in [\'vgg\', \'concat\']:\n-        vgg_ds, vgg_class_num = get_train_dataset(conf.vgg_folder/\'imgs\')\n-        print(\'vgg loader generated\')        \n-    if conf.data_mode == \'vgg\':\n-        ds = vgg_ds\n-        class_num = vgg_class_num\n-    elif conf.data_mode == \'ms1m\':\n-        ds = ms1m_ds\n-        class_num = ms1m_class_num\n-    elif conf.data_mode == \'concat\':\n-        for i,(url,label) in enumerate(vgg_ds.imgs):\n-            vgg_ds.imgs[i] = (url, label + ms1m_class_num)\n-        ds = ConcatDataset([ms1m_ds,vgg_ds])\n-        class_num = vgg_class_num + ms1m_class_num\n-    elif conf.data_mode == \'emore\':\n-        ds, class_num = get_train_dataset(conf.emore_folder/\'imgs\')\n-    loader = DataLoader(ds, batch_size=conf.batch_size, shuffle=True, pin_memory=conf.pin_memory, num_workers=conf.num_workers)\n-    return loader, class_num \n-    \n-def load_bin(path, rootdir, transform, image_size=[112,112]):\n-    if not rootdir.exists():\n-        rootdir.mkdir()\n-    bins, issame_list = pickle.load(open(path, \'rb\'), encoding=\'bytes\')\n-    data = bcolz.fill([len(bins), 3, image_size[0], image_size[1]], dtype=np.float32, rootdir=rootdir, mode=\'w\')\n-    for i in range(len(bins)):\n-        _bin = bins[i]\n-        img = mx.image.imdecode(_bin).asnumpy()\n-        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n-        img = Image.fromarray(img.astype(np.uint8))\n-        data[i, ...] = transform(img)\n-        i += 1\n-        if i % 1000 == 0:\n-            print(\'loading bin\', i)\n-    print(data.shape)\n-    np.save(str(rootdir)+\'_list\', np.array(issame_list))\n-    return data, issame_list\n-\n-def get_val_pair(path, name):\n-    carray = bcolz.carray(rootdir = path/name, mode=\'r\')\n-    issame = np.load(path/\'{}_list.npy\'.format(name))\n-    return carray, issame\n-\n-def get_val_data(data_path):\n-    agedb_30, agedb_30_issame = get_val_pair(data_path, \'agedb_30\')\n-    cfp_fp, cfp_fp_issame = get_val_pair(data_path, \'cfp_fp\')\n-    lfw, lfw_issame = get_val_pair(data_path, \'lfw\')\n-    return agedb_30, cfp_fp, lfw, agedb_30_issame, cfp_fp_issame, lfw_issame\n-\n-def load_mx_rec(rec_path):\n-    save_path = rec_path/\'imgs\'\n-    if not save_path.exists():\n-        save_path.mkdir()\n-    imgrec = mx.recordio.MXIndexedRecordIO(str(rec_path/\'train.idx\'), str(rec_path/\'train.rec\'), \'r\')\n-    img_info = imgrec.read_idx(0)\n-    header,_ = mx.recordio.unpack(img_info)\n-    max_idx = int(header.label[0])\n-    for idx in tqdm(range(1,max_idx)):\n-        img_info = imgrec.read_idx(idx)\n-        header, img = mx.recordio.unpack_img(img_info)\n-        label = int(header.label)\n-        img = Image.fromarray(img)\n-        label_path = save_path/str(label)\n-        if not label_path.exists():\n-            label_path.mkdir()\n-        img.save(label_path/\'{}.jpg\'.format(idx), quality=95)\n-\n-# class train_dataset(Dataset):\n-#     def __init__(self, imgs_bcolz, label_bcolz, h_flip=True):\n-#         self.imgs = bcolz.carray(rootdir = imgs_bcolz)\n-#         self.labels = bcolz.carray(rootdir = label_bcolz)\n-#         self.h_flip = h_flip\n-#         self.length = len(self.imgs) - 1\n-#         if h_flip:\n-#             self.transform = trans.Compose([\n-#                 trans.ToPILImage(),\n-#                 trans.RandomHorizontalFlip(),\n-#                 trans.ToTensor(),\n-#                 trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n-#             ])\n-#         self.class_num = self.labels[-1] + 1\n-        \n-#     def __len__(self):\n-#         return self.length\n-    \n-#     def __getitem__(self, index):\n-#         img = torch.tensor(self.imgs[index+1], dtype=torch.float)\n-#         label = torch.tensor(self.labels[index+1], dtype=torch.long)\n-#         if self.h_flip:\n-#             img = de_preprocess(img)\n-#             img = self.transform(img)\n-#         return img, label\n\\ No newline at end of file\ndiff --git a/data/facebank/Chandler/Chandler.02.png b/data/facebank/Chandler/Chandler.02.png\ndeleted file mode 100644\nindex e824825..0000000\nBinary files a/data/facebank/Chandler/Chandler.02.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.03.png b/data/facebank/Chandler/Chandler.03.png\ndeleted file mode 100644\nindex c8ad160..0000000\nBinary files a/data/facebank/Chandler/Chandler.03.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.04.png b/data/facebank/Chandler/Chandler.04.png\ndeleted file mode 100644\nindex cd13e74..0000000\nBinary files a/data/facebank/Chandler/Chandler.04.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.05.png b/data/facebank/Chandler/Chandler.05.png\ndeleted file mode 100644\nindex 6ac06fb..0000000\nBinary files a/data/facebank/Chandler/Chandler.05.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.06.png b/data/facebank/Chandler/Chandler.06.png\ndeleted file mode 100644\nindex e598bd4..0000000\nBinary files a/data/facebank/Chandler/Chandler.06.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.07.png b/data/facebank/Chandler/Chandler.07.png\ndeleted file mode 100644\nindex 6a3d942..0000000\nBinary files a/data/facebank/Chandler/Chandler.07.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.08.png b/data/facebank/Chandler/Chandler.08.png\ndeleted file mode 100644\nindex 15fdae4..0000000\nBinary files a/data/facebank/Chandler/Chandler.08.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.09.png b/data/facebank/Chandler/Chandler.09.png\ndeleted file mode 100644\nindex 962a80d..0000000\nBinary files a/data/facebank/Chandler/Chandler.09.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.10.png b/data/facebank/Chandler/Chandler.10.png\ndeleted file mode 100644\nindex bc7707c..0000000\nBinary files a/data/facebank/Chandler/Chandler.10.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.11.png b/data/facebank/Chandler/Chandler.11.png\ndeleted file mode 100644\nindex 08f3f50..0000000\nBinary files a/data/facebank/Chandler/Chandler.11.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.17.png b/data/facebank/Chandler/Chandler.17.png\ndeleted file mode 100644\nindex 0db2c6c..0000000\nBinary files a/data/facebank/Chandler/Chandler.17.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler.png b/data/facebank/Chandler/Chandler.png\ndeleted file mode 100644\nindex f998d6f..0000000\nBinary files a/data/facebank/Chandler/Chandler.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler12.png b/data/facebank/Chandler/Chandler12.png\ndeleted file mode 100644\nindex 4b5720a..0000000\nBinary files a/data/facebank/Chandler/Chandler12.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler13.png b/data/facebank/Chandler/Chandler13.png\ndeleted file mode 100644\nindex aea4252..0000000\nBinary files a/data/facebank/Chandler/Chandler13.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler14.png b/data/facebank/Chandler/Chandler14.png\ndeleted file mode 100644\nindex 5dfdbf3..0000000\nBinary files a/data/facebank/Chandler/Chandler14.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler15.png b/data/facebank/Chandler/Chandler15.png\ndeleted file mode 100644\nindex 4f1efef..0000000\nBinary files a/data/facebank/Chandler/Chandler15.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler16.png b/data/facebank/Chandler/Chandler16.png\ndeleted file mode 100644\nindex 987c894..0000000\nBinary files a/data/facebank/Chandler/Chandler16.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler18.png b/data/facebank/Chandler/Chandler18.png\ndeleted file mode 100644\nindex dc5576f..0000000\nBinary files a/data/facebank/Chandler/Chandler18.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler19.png b/data/facebank/Chandler/Chandler19.png\ndeleted file mode 100644\nindex e407adc..0000000\nBinary files a/data/facebank/Chandler/Chandler19.png and /dev/null differ\ndiff --git a/data/facebank/Chandler/Chandler20.png b/data/facebank/Chandler/Chandler20.png\ndeleted file mode 100644\nindex 6fc1ab3..0000000\nBinary files a/data/facebank/Chandler/Chandler20.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.01.png b/data/facebank/Joey/Joey.01.png\ndeleted file mode 100644\nindex 3c685d0..0000000\nBinary files a/data/facebank/Joey/Joey.01.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.02.png b/data/facebank/Joey/Joey.02.png\ndeleted file mode 100644\nindex 9ca5128..0000000\nBinary files a/data/facebank/Joey/Joey.02.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.03.png b/data/facebank/Joey/Joey.03.png\ndeleted file mode 100644\nindex 6d2caca..0000000\nBinary files a/data/facebank/Joey/Joey.03.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.04.png b/data/facebank/Joey/Joey.04.png\ndeleted file mode 100644\nindex f698017..0000000\nBinary files a/data/facebank/Joey/Joey.04.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.05.png b/data/facebank/Joey/Joey.05.png\ndeleted file mode 100644\nindex 11c4831..0000000\nBinary files a/data/facebank/Joey/Joey.05.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.06.png b/data/facebank/Joey/Joey.06.png\ndeleted file mode 100644\nindex 96f7ed3..0000000\nBinary files a/data/facebank/Joey/Joey.06.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.07.png b/data/facebank/Joey/Joey.07.png\ndeleted file mode 100644\nindex 1e165fa..0000000\nBinary files a/data/facebank/Joey/Joey.07.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.08.png b/data/facebank/Joey/Joey.08.png\ndeleted file mode 100644\nindex 09144d8..0000000\nBinary files a/data/facebank/Joey/Joey.08.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.09.png b/data/facebank/Joey/Joey.09.png\ndeleted file mode 100644\nindex 7ed417e..0000000\nBinary files a/data/facebank/Joey/Joey.09.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.11.png b/data/facebank/Joey/Joey.11.png\ndeleted file mode 100644\nindex 580bc53..0000000\nBinary files a/data/facebank/Joey/Joey.11.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.12.png b/data/facebank/Joey/Joey.12.png\ndeleted file mode 100644\nindex f298964..0000000\nBinary files a/data/facebank/Joey/Joey.12.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.13.png b/data/facebank/Joey/Joey.13.png\ndeleted file mode 100644\nindex 0498bc2..0000000\nBinary files a/data/facebank/Joey/Joey.13.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.14.png b/data/facebank/Joey/Joey.14.png\ndeleted file mode 100644\nindex 54b8ddc..0000000\nBinary files a/data/facebank/Joey/Joey.14.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.15.png b/data/facebank/Joey/Joey.15.png\ndeleted file mode 100644\nindex c2a4757..0000000\nBinary files a/data/facebank/Joey/Joey.15.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.16.png b/data/facebank/Joey/Joey.16.png\ndeleted file mode 100644\nindex e3bc9b8..0000000\nBinary files a/data/facebank/Joey/Joey.16.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.17.png b/data/facebank/Joey/Joey.17.png\ndeleted file mode 100644\nindex 4e9312e..0000000\nBinary files a/data/facebank/Joey/Joey.17.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.18.png b/data/facebank/Joey/Joey.18.png\ndeleted file mode 100644\nindex 936d8a5..0000000\nBinary files a/data/facebank/Joey/Joey.18.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.19.png b/data/facebank/Joey/Joey.19.png\ndeleted file mode 100644\nindex bdfb07d..0000000\nBinary files a/data/facebank/Joey/Joey.19.png and /dev/null differ\ndiff --git a/data/facebank/Joey/Joey.20.png b/data/facebank/Joey/Joey.20.png\ndeleted file mode 100644\nindex b78b6bf..0000000\nBinary files a/data/facebank/Joey/Joey.20.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.02.png b/data/facebank/Monica/Monica.02.png\ndeleted file mode 100644\nindex 1a05511..0000000\nBinary files a/data/facebank/Monica/Monica.02.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.03.png b/data/facebank/Monica/Monica.03.png\ndeleted file mode 100644\nindex 1f58157..0000000\nBinary files a/data/facebank/Monica/Monica.03.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.04.png b/data/facebank/Monica/Monica.04.png\ndeleted file mode 100644\nindex 05a254a..0000000\nBinary files a/data/facebank/Monica/Monica.04.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.05.png b/data/facebank/Monica/Monica.05.png\ndeleted file mode 100644\nindex f97be0d..0000000\nBinary files a/data/facebank/Monica/Monica.05.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.08.png b/data/facebank/Monica/Monica.08.png\ndeleted file mode 100644\nindex 4ccbc47..0000000\nBinary files a/data/facebank/Monica/Monica.08.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica.png b/data/facebank/Monica/Monica.png\ndeleted file mode 100644\nindex 758c3c6..0000000\nBinary files a/data/facebank/Monica/Monica.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica10.png b/data/facebank/Monica/Monica10.png\ndeleted file mode 100644\nindex 8994753..0000000\nBinary files a/data/facebank/Monica/Monica10.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica11.png b/data/facebank/Monica/Monica11.png\ndeleted file mode 100644\nindex 357b8bc..0000000\nBinary files a/data/facebank/Monica/Monica11.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica12.png b/data/facebank/Monica/Monica12.png\ndeleted file mode 100644\nindex dc9ac29..0000000\nBinary files a/data/facebank/Monica/Monica12.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica13.png b/data/facebank/Monica/Monica13.png\ndeleted file mode 100644\nindex d7c2cf6..0000000\nBinary files a/data/facebank/Monica/Monica13.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica14.png b/data/facebank/Monica/Monica14.png\ndeleted file mode 100644\nindex a528414..0000000\nBinary files a/data/facebank/Monica/Monica14.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica15.png b/data/facebank/Monica/Monica15.png\ndeleted file mode 100644\nindex 5bfe1a9..0000000\nBinary files a/data/facebank/Monica/Monica15.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica16.png b/data/facebank/Monica/Monica16.png\ndeleted file mode 100644\nindex f94e645..0000000\nBinary files a/data/facebank/Monica/Monica16.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica17.png b/data/facebank/Monica/Monica17.png\ndeleted file mode 100644\nindex bf24311..0000000\nBinary files a/data/facebank/Monica/Monica17.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica18.png b/data/facebank/Monica/Monica18.png\ndeleted file mode 100644\nindex b04f261..0000000\nBinary files a/data/facebank/Monica/Monica18.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica19.png b/data/facebank/Monica/Monica19.png\ndeleted file mode 100644\nindex 0eeb0d6..0000000\nBinary files a/data/facebank/Monica/Monica19.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica20.png b/data/facebank/Monica/Monica20.png\ndeleted file mode 100644\nindex e5d4e24..0000000\nBinary files a/data/facebank/Monica/Monica20.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica6.png b/data/facebank/Monica/Monica6.png\ndeleted file mode 100644\nindex 7112741..0000000\nBinary files a/data/facebank/Monica/Monica6.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica7.png b/data/facebank/Monica/Monica7.png\ndeleted file mode 100644\nindex 3567770..0000000\nBinary files a/data/facebank/Monica/Monica7.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica8.png b/data/facebank/Monica/Monica8.png\ndeleted file mode 100644\nindex de723c2..0000000\nBinary files a/data/facebank/Monica/Monica8.png and /dev/null differ\ndiff --git a/data/facebank/Monica/Monica9.png b/data/facebank/Monica/Monica9.png\ndeleted file mode 100644\nindex c7ae768..0000000\nBinary files a/data/facebank/Monica/Monica9.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe06.png b/data/facebank/Phoebe/Pheobe06.png\ndeleted file mode 100644\nindex 3270a78..0000000\nBinary files a/data/facebank/Phoebe/Pheobe06.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe07.png b/data/facebank/Phoebe/Pheobe07.png\ndeleted file mode 100644\nindex 25cb4fd..0000000\nBinary files a/data/facebank/Phoebe/Pheobe07.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe08.png b/data/facebank/Phoebe/Pheobe08.png\ndeleted file mode 100644\nindex cd5d7e0..0000000\nBinary files a/data/facebank/Phoebe/Pheobe08.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe09.png b/data/facebank/Phoebe/Pheobe09.png\ndeleted file mode 100644\nindex 4df4c47..0000000\nBinary files a/data/facebank/Phoebe/Pheobe09.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe10.png b/data/facebank/Phoebe/Pheobe10.png\ndeleted file mode 100644\nindex 8637633..0000000\nBinary files a/data/facebank/Phoebe/Pheobe10.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe11.png b/data/facebank/Phoebe/Pheobe11.png\ndeleted file mode 100644\nindex 7ad6ed9..0000000\nBinary files a/data/facebank/Phoebe/Pheobe11.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe12.png b/data/facebank/Phoebe/Pheobe12.png\ndeleted file mode 100644\nindex 7ce09dc..0000000\nBinary files a/data/facebank/Phoebe/Pheobe12.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe13.png b/data/facebank/Phoebe/Pheobe13.png\ndeleted file mode 100644\nindex 17b6c11..0000000\nBinary files a/data/facebank/Phoebe/Pheobe13.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe15.png b/data/facebank/Phoebe/Pheobe15.png\ndeleted file mode 100644\nindex 6c47f10..0000000\nBinary files a/data/facebank/Phoebe/Pheobe15.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe16.png b/data/facebank/Phoebe/Pheobe16.png\ndeleted file mode 100644\nindex 56775ea..0000000\nBinary files a/data/facebank/Phoebe/Pheobe16.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe17.png b/data/facebank/Phoebe/Pheobe17.png\ndeleted file mode 100644\nindex 0897eb9..0000000\nBinary files a/data/facebank/Phoebe/Pheobe17.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe18.png b/data/facebank/Phoebe/Pheobe18.png\ndeleted file mode 100644\nindex 2579394..0000000\nBinary files a/data/facebank/Phoebe/Pheobe18.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe19.png b/data/facebank/Phoebe/Pheobe19.png\ndeleted file mode 100644\nindex 111693b..0000000\nBinary files a/data/facebank/Phoebe/Pheobe19.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Pheobe20.png b/data/facebank/Phoebe/Pheobe20.png\ndeleted file mode 100644\nindex 82ecd08..0000000\nBinary files a/data/facebank/Phoebe/Pheobe20.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.02.png b/data/facebank/Phoebe/Phoebe.02.png\ndeleted file mode 100644\nindex 5a67bbc..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.02.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.03.png b/data/facebank/Phoebe/Phoebe.03.png\ndeleted file mode 100644\nindex 43a1ae2..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.03.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.04.png b/data/facebank/Phoebe/Phoebe.04.png\ndeleted file mode 100644\nindex e221b0d..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.04.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.05.png b/data/facebank/Phoebe/Phoebe.05.png\ndeleted file mode 100644\nindex 96a6629..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.05.png and /dev/null differ\ndiff --git a/data/facebank/Phoebe/Phoebe.png b/data/facebank/Phoebe/Phoebe.png\ndeleted file mode 100644\nindex befbda6..0000000\nBinary files a/data/facebank/Phoebe/Phoebe.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_1.png b/data/facebank/Pias/Pias_1.png\ndeleted file mode 100644\nindex 6fd8a18..0000000\nBinary files a/data/facebank/Pias/Pias_1.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_10.png b/data/facebank/Pias/Pias_10.png\ndeleted file mode 100644\nindex 7bfc6fa..0000000\nBinary files a/data/facebank/Pias/Pias_10.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_11.png b/data/facebank/Pias/Pias_11.png\ndeleted file mode 100644\nindex 4dc9bcd..0000000\nBinary files a/data/facebank/Pias/Pias_11.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_12.png b/data/facebank/Pias/Pias_12.png\ndeleted file mode 100644\nindex 46c1829..0000000\nBinary files a/data/facebank/Pias/Pias_12.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_13.png b/data/facebank/Pias/Pias_13.png\ndeleted file mode 100644\nindex 5debf3e..0000000\nBinary files a/data/facebank/Pias/Pias_13.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_14.png b/data/facebank/Pias/Pias_14.png\ndeleted file mode 100644\nindex d511edf..0000000\nBinary files a/data/facebank/Pias/Pias_14.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_15.png b/data/facebank/Pias/Pias_15.png\ndeleted file mode 100644\nindex 9cc3ab2..0000000\nBinary files a/data/facebank/Pias/Pias_15.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_16.png b/data/facebank/Pias/Pias_16.png\ndeleted file mode 100644\nindex d143958..0000000\nBinary files a/data/facebank/Pias/Pias_16.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_2.png b/data/facebank/Pias/Pias_2.png\ndeleted file mode 100644\nindex 6341f45..0000000\nBinary files a/data/facebank/Pias/Pias_2.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_3.png b/data/facebank/Pias/Pias_3.png\ndeleted file mode 100644\nindex 049432a..0000000\nBinary files a/data/facebank/Pias/Pias_3.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_4.png b/data/facebank/Pias/Pias_4.png\ndeleted file mode 100644\nindex 7ed7123..0000000\nBinary files a/data/facebank/Pias/Pias_4.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_5.png b/data/facebank/Pias/Pias_5.png\ndeleted file mode 100644\nindex 8c6723b..0000000\nBinary files a/data/facebank/Pias/Pias_5.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_6.png b/data/facebank/Pias/Pias_6.png\ndeleted file mode 100644\nindex 86e69e9..0000000\nBinary files a/data/facebank/Pias/Pias_6.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_7.png b/data/facebank/Pias/Pias_7.png\ndeleted file mode 100644\nindex b51eff1..0000000\nBinary files a/data/facebank/Pias/Pias_7.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_8.png b/data/facebank/Pias/Pias_8.png\ndeleted file mode 100644\nindex 35b643b..0000000\nBinary files a/data/facebank/Pias/Pias_8.png and /dev/null differ\ndiff --git a/data/facebank/Pias/Pias_9.png b/data/facebank/Pias/Pias_9.png\ndeleted file mode 100644\nindex 0d49133..0000000\nBinary files a/data/facebank/Pias/Pias_9.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.02.png b/data/facebank/Rachel/Rachel.02.png\ndeleted file mode 100644\nindex e62a99a..0000000\nBinary files a/data/facebank/Rachel/Rachel.02.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.03.png b/data/facebank/Rachel/Rachel.03.png\ndeleted file mode 100644\nindex f4ac8fa..0000000\nBinary files a/data/facebank/Rachel/Rachel.03.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.04.png b/data/facebank/Rachel/Rachel.04.png\ndeleted file mode 100644\nindex 1ff0d0c..0000000\nBinary files a/data/facebank/Rachel/Rachel.04.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel.05.png b/data/facebank/Rachel/Rachel.05.png\ndeleted file mode 100644\nindex 2d69283..0000000\nBinary files a/data/facebank/Rachel/Rachel.05.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel01.png b/data/facebank/Rachel/Rachel01.png\ndeleted file mode 100644\nindex 91e0fd9..0000000\nBinary files a/data/facebank/Rachel/Rachel01.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel06.png b/data/facebank/Rachel/Rachel06.png\ndeleted file mode 100644\nindex be80ce0..0000000\nBinary files a/data/facebank/Rachel/Rachel06.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel07.png b/data/facebank/Rachel/Rachel07.png\ndeleted file mode 100644\nindex 7f1c0d8..0000000\nBinary files a/data/facebank/Rachel/Rachel07.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel08.png b/data/facebank/Rachel/Rachel08.png\ndeleted file mode 100644\nindex 2295162..0000000\nBinary files a/data/facebank/Rachel/Rachel08.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel09.png b/data/facebank/Rachel/Rachel09.png\ndeleted file mode 100644\nindex 49dfe08..0000000\nBinary files a/data/facebank/Rachel/Rachel09.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel10.png b/data/facebank/Rachel/Rachel10.png\ndeleted file mode 100644\nindex bae25a1..0000000\nBinary files a/data/facebank/Rachel/Rachel10.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel11.png b/data/facebank/Rachel/Rachel11.png\ndeleted file mode 100644\nindex 0e39cef..0000000\nBinary files a/data/facebank/Rachel/Rachel11.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel12.png b/data/facebank/Rachel/Rachel12.png\ndeleted file mode 100644\nindex d0945d5..0000000\nBinary files a/data/facebank/Rachel/Rachel12.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel13.png b/data/facebank/Rachel/Rachel13.png\ndeleted file mode 100644\nindex 9a942f2..0000000\nBinary files a/data/facebank/Rachel/Rachel13.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel14.png b/data/facebank/Rachel/Rachel14.png\ndeleted file mode 100644\nindex 3786268..0000000\nBinary files a/data/facebank/Rachel/Rachel14.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel15.png b/data/facebank/Rachel/Rachel15.png\ndeleted file mode 100644\nindex c8b5b67..0000000\nBinary files a/data/facebank/Rachel/Rachel15.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel16.png b/data/facebank/Rachel/Rachel16.png\ndeleted file mode 100644\nindex 8fa13b5..0000000\nBinary files a/data/facebank/Rachel/Rachel16.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel17.png b/data/facebank/Rachel/Rachel17.png\ndeleted file mode 100644\nindex dcd563d..0000000\nBinary files a/data/facebank/Rachel/Rachel17.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel18.png b/data/facebank/Rachel/Rachel18.png\ndeleted file mode 100644\nindex 5d58bc1..0000000\nBinary files a/data/facebank/Rachel/Rachel18.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel19.png b/data/facebank/Rachel/Rachel19.png\ndeleted file mode 100644\nindex 5f9778a..0000000\nBinary files a/data/facebank/Rachel/Rachel19.png and /dev/null differ\ndiff --git a/data/facebank/Rachel/Rachel20.png b/data/facebank/Rachel/Rachel20.png\ndeleted file mode 100644\nindex 7d62102..0000000\nBinary files a/data/facebank/Rachel/Rachel20.png and /dev/null differ\ndiff --git a/data/facebank/Raihan/Raihan_01.png b/data/facebank/Raihan/Raihan_01.png\ndeleted file mode 100644\nindex 81ba916..0000000\nBinary files a/data/facebank/Raihan/Raihan_01.png and /dev/null differ\ndiff --git a/data/facebank/Raihan/Raihan_02.png b/data/facebank/Raihan/Raihan_02.png\ndeleted file mode 100644\nindex b9f040e..0000000\nBinary files a/data/facebank/Raihan/Raihan_02.png and /dev/null differ\ndiff --git a/data/facebank/Ross/Ross03.png b/data/facebank/Ross/Ross03.png\ndeleted file mode 100644\nindex d8c2c7d..0000000\nBinary files a/data/facebank/Ross/Ross03.png and /dev/null differ\ndiff --git a/data/facebank/Ross/Ross04.png b/data/facebank/Ross/Ross04.png\ndeleted file mode 100644\nindex b3ccc1b..0000000\nBinary files a/data/facebank/Ross/Ross04.png and /dev/null differ\ndiff --git a/data/facebank/Ross/Ross05.png b/data/facebank/Ross/Ross05.png\ndeleted file mode 100644\nindex 95fda3f..0000000\nBinary files a/data/facebank/Ross/Ross05.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross01.png b/data/facebank/Ross/ross01.png\ndeleted file mode 100644\nindex 3459b69..0000000\nBinary files a/data/facebank/Ross/ross01.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross02.png b/data/facebank/Ross/ross02.png\ndeleted file mode 100644\nindex 332c896..0000000\nBinary files a/data/facebank/Ross/ross02.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross06.png b/data/facebank/Ross/ross06.png\ndeleted file mode 100644\nindex 8cc4528..0000000\nBinary files a/data/facebank/Ross/ross06.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross07.png b/data/facebank/Ross/ross07.png\ndeleted file mode 100644\nindex f26cf1d..0000000\nBinary files a/data/facebank/Ross/ross07.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross08.png b/data/facebank/Ross/ross08.png\ndeleted file mode 100644\nindex 76b0a88..0000000\nBinary files a/data/facebank/Ross/ross08.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross09.png b/data/facebank/Ross/ross09.png\ndeleted file mode 100644\nindex 50d07c8..0000000\nBinary files a/data/facebank/Ross/ross09.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross10.png b/data/facebank/Ross/ross10.png\ndeleted file mode 100644\nindex 64651c2..0000000\nBinary files a/data/facebank/Ross/ross10.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross11.png b/data/facebank/Ross/ross11.png\ndeleted file mode 100644\nindex bfc016b..0000000\nBinary files a/data/facebank/Ross/ross11.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross12.png b/data/facebank/Ross/ross12.png\ndeleted file mode 100644\nindex d65cc34..0000000\nBinary files a/data/facebank/Ross/ross12.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross13.png b/data/facebank/Ross/ross13.png\ndeleted file mode 100644\nindex 2671298..0000000\nBinary files a/data/facebank/Ross/ross13.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross14.png b/data/facebank/Ross/ross14.png\ndeleted file mode 100644\nindex c239eea..0000000\nBinary files a/data/facebank/Ross/ross14.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross15.png b/data/facebank/Ross/ross15.png\ndeleted file mode 100644\nindex 7326319..0000000\nBinary files a/data/facebank/Ross/ross15.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross16.png b/data/facebank/Ross/ross16.png\ndeleted file mode 100644\nindex 7786cdd..0000000\nBinary files a/data/facebank/Ross/ross16.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross17.png b/data/facebank/Ross/ross17.png\ndeleted file mode 100644\nindex 0c70f90..0000000\nBinary files a/data/facebank/Ross/ross17.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross18.png b/data/facebank/Ross/ross18.png\ndeleted file mode 100644\nindex 06d916d..0000000\nBinary files a/data/facebank/Ross/ross18.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross19.png b/data/facebank/Ross/ross19.png\ndeleted file mode 100644\nindex 8f79393..0000000\nBinary files a/data/facebank/Ross/ross19.png and /dev/null differ\ndiff --git a/data/facebank/Ross/ross20.png b/data/facebank/Ross/ross20.png\ndeleted file mode 100644\nindex 6621203..0000000\nBinary files a/data/facebank/Ross/ross20.png and /dev/null differ\ndiff --git a/data/facebank/Samiur/Samiur_01.png b/data/facebank/Samiur/Samiur_01.png\ndeleted file mode 100644\nindex 9bac23b..0000000\nBinary files a/data/facebank/Samiur/Samiur_01.png and /dev/null differ\ndiff --git a/data/facebank/Samiur/Samiur_02.png b/data/facebank/Samiur/Samiur_02.png\ndeleted file mode 100644\nindex 6ab6e02..0000000\nBinary files a/data/facebank/Samiur/Samiur_02.png and /dev/null differ\ndiff --git a/data/facebank/Samiur/Samiur_03.png b/data/facebank/Samiur/Samiur_03.png\ndeleted file mode 100644\nindex 9aadb6c..0000000\nBinary files a/data/facebank/Samiur/Samiur_03.png and /dev/null differ\ndiff --git a/data/facebank/Shakil/Shakil1.png b/data/facebank/Shakil/Shakil1.png\ndeleted file mode 100644\nindex 4000a1e..0000000\nBinary files a/data/facebank/Shakil/Shakil1.png and /dev/null differ\ndiff --git a/data/facebank/Shakil/Shakil2.png b/data/facebank/Shakil/Shakil2.png\ndeleted file mode 100644\nindex 972844f..0000000\nBinary files a/data/facebank/Shakil/Shakil2.png and /dev/null differ\ndiff --git a/data/facebank/Shakil/Shakil3.png b/data/facebank/Shakil/Shakil3.png\ndeleted file mode 100644\nindex bc60c41..0000000\nBinary files a/data/facebank/Shakil/Shakil3.png and /dev/null differ\ndiff --git a/data/facebank/Shakil/Shakil4.png b/data/facebank/Shakil/Shakil4.png\ndeleted file mode 100644\nindex 20457c7..0000000\nBinary files a/data/facebank/Shakil/Shakil4.png and /dev/null differ\ndiff --git a/data/facebank/Shakil/Shakil5.png b/data/facebank/Shakil/Shakil5.png\ndeleted file mode 100644\nindex 6db07ba..0000000\nBinary files a/data/facebank/Shakil/Shakil5.png and /dev/null differ\ndiff --git a/data/facebank/Shakil/Shakil6.png b/data/facebank/Shakil/Shakil6.png\ndeleted file mode 100644\nindex 9c4b5f2..0000000\nBinary files a/data/facebank/Shakil/Shakil6.png and /dev/null differ\ndiff --git a/data/facebank/Shakil/Shakil7.png b/data/facebank/Shakil/Shakil7.png\ndeleted file mode 100644\nindex 8f45955..0000000\nBinary files a/data/facebank/Shakil/Shakil7.png and /dev/null differ\ndiff --git a/data/facebank/facebank.pth b/data/facebank/facebank.pth\ndeleted file mode 100644\nindex b421fe1..0000000\n--- a/data/facebank/facebank.pth\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-version https://git-lfs.github.com/spec/v1\n-oid sha256:12f2be55177b222cedc8853565bffddfa1343f5b64fd516ea4f87924210eeb7d\n-size 20825\ndiff --git a/data/facebank/names.npy b/data/facebank/names.npy\ndeleted file mode 100644\nindex 6462009..0000000\nBinary files a/data/facebank/names.npy and /dev/null differ\ndiff --git a/face_verify.py b/face_verify.py\ndeleted file mode 100644\nindex 03289ec..0000000\n--- a/face_verify.py\n+++ /dev/null\n@@ -1,88 +0,0 @@\n-import cv2\n-from PIL import Image\n-import argparse\n-from pathlib import Path\n-from multiprocessing import Process, Pipe,Value,Array\n-import torch\n-from config import get_config\n-from mtcnn import MTCNN\n-from Learner import face_learner\n-from utils import load_facebank, draw_box_name, prepare_facebank\n-\n-\n-parser = argparse.ArgumentParser(description=\'for face verification\')\n-parser.add_argument("-s", "--save", help="whether save",action="store_true")\n-parser.add_argument(\'-th\',\'--threshold\',help=\'threshold to decide identical faces\',default=1.54, type=float)\n-parser.add_argument("-u", "--update", help="whether perform update the facebank",action="store_true")\n-parser.add_argument("-tta", "--tta", help="whether test time augmentation",action="store_true")\n-parser.add_argument("-c", "--score", help="whether show the confidence score",action="store_true")\n-args = parser.parse_args()\n-\n-conf = get_config(False)\n-\n-mtcnn = MTCNN()\n-print(\'arcface loaded\')\n-\n-learner = face_learner(conf, True)\n-learner.threshold = args.threshold\n-if conf.device.type == \'cpu\':\n-    learner.load_state(conf, \'cpu_final.pth\', True, True)\n-else:\n-    learner.load_state(conf, \'final.pth\', True, True)\n-learner.model.eval()\n-print(\'learner loaded\')\n-\n-if args.update:\n-    targets, names = prepare_facebank(conf, learner.model, mtcnn, tta = args.tta)\n-    print(\'facebank updated\')\n-else:\n-    targets, names = load_facebank(conf)\n-    print(\'facebank loaded\')\n-\n-# inital camera\n-\n-cap = cv2.VideoCapture(0)\n-cap.set(3,500)\n-cap.set(4,500)\n-\n-\n-class faceRec:\n-    def __init__(self):\n-        self.width = 800\n-        self.height = 800\n-        self.image = None\n-    def main(self): \n-        while cap.isOpened():\n-            isSuccess,frame = cap.read()\n-            if isSuccess:            \n-                try:\n-    #                 image = Image.fromarray(frame[...,::-1]) #bgr to rgb\n-                    image = Image.fromarray(frame)\n-                    bboxes, faces = mtcnn.align_multi(image, conf.face_limit, conf.min_face_size)\n-                    bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\n-                    bboxes = bboxes.astype(int)\n-                    bboxes = bboxes + [-1,-1,1,1] # personal choice    \n-                    results, score = learner.infer(conf, faces, targets, args.tta)\n-                    # print(score[0])\n-                    for idx,bbox in enumerate(bboxes):\n-                        if args.score:\n-                            frame = draw_box_name(bbox, names[results[idx] + 1] + \'_{:.2f}\'.format(score[idx]), frame)\n-                        else:\n-                            if float(\'{:.2f}\'.format(score[idx])) > .98:\n-                                name = names[0]\n-                            else:    \n-                                name = names[results[idx]+1]\n-                            frame = draw_box_name(bbox, names[results[idx] + 1], frame)\n-                except:\n-                    pass    \n-                ret, jpeg = cv2.imencode(\'.jpg\', frame)\n-                return jpeg.tostring()\n-                # cv2.imshow(\'Arc Face Recognizer\', frame)\n-\n-\n-            if cv2.waitKey(1)&0xFF == ord(\'q\'):\n-                break\n-\n-        cap.release()\n-\n-        cv2.destroyAllWindows()    \ndiff --git a/model.py b/model.py\ndeleted file mode 100644\nindex 6645455..0000000\n--- a/model.py\n+++ /dev/null\n@@ -1,306 +0,0 @@\n-from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n-import torch.nn.functional as F\n-import torch\n-from collections import namedtuple\n-import math\n-import pdb\n-\n-##################################  Original Arcface Model #############################################################\n-\n-class Flatten(Module):\n-    def forward(self, input):\n-        return input.view(input.size(0), -1)\n-\n-def l2_norm(input,axis=1):\n-    norm = torch.norm(input,2,axis,True)\n-    output = torch.div(input, norm)\n-    return output\n-\n-class SEModule(Module):\n-    def __init__(self, channels, reduction):\n-        super(SEModule, self).__init__()\n-        self.avg_pool = AdaptiveAvgPool2d(1)\n-        self.fc1 = Conv2d(\n-            channels, channels // reduction, kernel_size=1, padding=0 ,bias=False)\n-        self.relu = ReLU(inplace=True)\n-        self.fc2 = Conv2d(\n-            channels // reduction, channels, kernel_size=1, padding=0 ,bias=False)\n-        self.sigmoid = Sigmoid()\n-\n-    def forward(self, x):\n-        module_input = x\n-        x = self.avg_pool(x)\n-        x = self.fc1(x)\n-        x = self.relu(x)\n-        x = self.fc2(x)\n-        x = self.sigmoid(x)\n-        return module_input * x\n-\n-class bottleneck_IR(Module):\n-    def __init__(self, in_channel, depth, stride):\n-        super(bottleneck_IR, self).__init__()\n-        if in_channel == depth:\n-            self.shortcut_layer = MaxPool2d(1, stride)\n-        else:\n-            self.shortcut_layer = Sequential(\n-                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), BatchNorm2d(depth))\n-        self.res_layer = Sequential(\n-            BatchNorm2d(in_channel),\n-            Conv2d(in_channel, depth, (3, 3), (1, 1), 1 ,bias=False), PReLU(depth),\n-            Conv2d(depth, depth, (3, 3), stride, 1 ,bias=False), BatchNorm2d(depth))\n-\n-    def forward(self, x):\n-        shortcut = self.shortcut_layer(x)\n-        res = self.res_layer(x)\n-        return res + shortcut\n-\n-class bottleneck_IR_SE(Module):\n-    def __init__(self, in_channel, depth, stride):\n-        super(bottleneck_IR_SE, self).__init__()\n-        if in_channel == depth:\n-            self.shortcut_layer = MaxPool2d(1, stride)\n-        else:\n-            self.shortcut_layer = Sequential(\n-                Conv2d(in_channel, depth, (1, 1), stride ,bias=False), \n-                BatchNorm2d(depth))\n-        self.res_layer = Sequential(\n-            BatchNorm2d(in_channel),\n-            Conv2d(in_channel, depth, (3,3), (1,1),1 ,bias=False),\n-            PReLU(depth),\n-            Conv2d(depth, depth, (3,3), stride, 1 ,bias=False),\n-            BatchNorm2d(depth),\n-            SEModule(depth,16)\n-            )\n-    def forward(self,x):\n-        shortcut = self.shortcut_layer(x)\n-        res = self.res_layer(x)\n-        return res + shortcut\n-\n-class Bottleneck(namedtuple(\'Block\', [\'in_channel\', \'depth\', \'stride\'])):\n-    \'\'\'A named tuple describing a ResNet block.\'\'\'\n-    \n-def get_block(in_channel, depth, num_units, stride = 2):\n-    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units-1)]\n-\n-def get_blocks(num_layers):\n-    if num_layers == 50:\n-        blocks = [\n-            get_block(in_channel=64, depth=64, num_units = 3),\n-            get_block(in_channel=64, depth=128, num_units=4),\n-            get_block(in_channel=128, depth=256, num_units=14),\n-            get_block(in_channel=256, depth=512, num_units=3)\n-        ]\n-    elif num_layers == 100:\n-        blocks = [\n-            get_block(in_channel=64, depth=64, num_units=3),\n-            get_block(in_channel=64, depth=128, num_units=13),\n-            get_block(in_channel=128, depth=256, num_units=30),\n-            get_block(in_channel=256, depth=512, num_units=3)\n-        ]\n-    elif num_layers == 152:\n-        blocks = [\n-            get_block(in_channel=64, depth=64, num_units=3),\n-            get_block(in_channel=64, depth=128, num_units=8),\n-            get_block(in_channel=128, depth=256, num_units=36),\n-            get_block(in_channel=256, depth=512, num_units=3)\n-        ]\n-    return blocks\n-\n-class Backbone(Module):\n-    def __init__(self, num_layers, drop_ratio, mode=\'ir\'):\n-        super(Backbone, self).__init__()\n-        assert num_layers in [50, 100, 152], \'num_layers should be 50,100, or 152\'\n-        assert mode in [\'ir\', \'ir_se\'], \'mode should be ir or ir_se\'\n-        blocks = get_blocks(num_layers)\n-        if mode == \'ir\':\n-            unit_module = bottleneck_IR\n-        elif mode == \'ir_se\':\n-            unit_module = bottleneck_IR_SE\n-        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1 ,bias=False), \n-                                      BatchNorm2d(64), \n-                                      PReLU(64))\n-        self.output_layer = Sequential(BatchNorm2d(512), \n-                                       Dropout(drop_ratio),\n-                                       Flatten(),\n-                                       Linear(512 * 7 * 7, 512),\n-                                       BatchNorm1d(512))\n-        modules = []\n-        for block in blocks:\n-            for bottleneck in block:\n-                modules.append(\n-                    unit_module(bottleneck.in_channel,\n-                                bottleneck.depth,\n-                                bottleneck.stride))\n-        self.body = Sequential(*modules)\n-    \n-    def forward(self,x):\n-        x = self.input_layer(x)\n-        x = self.body(x)\n-        x = self.output_layer(x)\n-        return l2_norm(x)\n-\n-##################################  MobileFaceNet #############################################################\n-    \n-class Conv_block(Module):\n-    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n-        super(Conv_block, self).__init__()\n-        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n-        self.bn = BatchNorm2d(out_c)\n-        self.prelu = PReLU(out_c)\n-    def forward(self, x):\n-        x = self.conv(x)\n-        x = self.bn(x)\n-        x = self.prelu(x)\n-        return x\n-\n-class Linear_block(Module):\n-    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n-        super(Linear_block, self).__init__()\n-        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n-        self.bn = BatchNorm2d(out_c)\n-    def forward(self, x):\n-        x = self.conv(x)\n-        x = self.bn(x)\n-        return x\n-\n-class Depth_Wise(Module):\n-     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n-        super(Depth_Wise, self).__init__()\n-        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n-        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n-        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n-        self.residual = residual\n-     def forward(self, x):\n-        if self.residual:\n-            short_cut = x\n-        x = self.conv(x)\n-        x = self.conv_dw(x)\n-        x = self.project(x)\n-        if self.residual:\n-            output = short_cut + x\n-        else:\n-            output = x\n-        return output\n-\n-class Residual(Module):\n-    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n-        super(Residual, self).__init__()\n-        modules = []\n-        for _ in range(num_block):\n-            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n-        self.model = Sequential(*modules)\n-    def forward(self, x):\n-        return self.model(x)\n-\n-class MobileFaceNet(Module):\n-    def __init__(self, embedding_size):\n-        super(MobileFaceNet, self).__init__()\n-        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n-        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n-        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n-        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n-        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n-        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n-        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n-        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n-        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n-        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n-        self.conv_6_flatten = Flatten()\n-        self.linear = Linear(512, embedding_size, bias=False)\n-        self.bn = BatchNorm1d(embedding_size)\n-    \n-    def forward(self, x):\n-        out = self.conv1(x)\n-\n-        out = self.conv2_dw(out)\n-\n-        out = self.conv_23(out)\n-\n-        out = self.conv_3(out)\n-        \n-        out = self.conv_34(out)\n-\n-        out = self.conv_4(out)\n-\n-        out = self.conv_45(out)\n-\n-        out = self.conv_5(out)\n-\n-        out = self.conv_6_sep(out)\n-\n-        out = self.conv_6_dw(out)\n-\n-        out = self.conv_6_flatten(out)\n-\n-        out = self.linear(out)\n-\n-        out = self.bn(out)\n-        return l2_norm(out)\n-\n-##################################  Arcface head #############################################################\n-\n-class Arcface(Module):\n-    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \n-    def __init__(self, embedding_size=512, classnum=51332,  s=64., m=0.5):\n-        super(Arcface, self).__init__()\n-        self.classnum = classnum\n-        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n-        # initial kernel\n-        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n-        self.m = m # the margin value, default is 0.5\n-        self.s = s # scalar value default is 64, see normface https://arxiv.org/abs/1704.06369\n-        self.cos_m = math.cos(m)\n-        self.sin_m = math.sin(m)\n-        self.mm = self.sin_m * m  # issue 1\n-        self.threshold = math.cos(math.pi - m)\n-    def forward(self, embbedings, label):\n-        # weights norm\n-        nB = len(embbedings)\n-        kernel_norm = l2_norm(self.kernel,axis=0)\n-        # cos(theta+m)\n-        cos_theta = torch.mm(embbedings,kernel_norm)\n-#         output = torch.mm(embbedings,kernel_norm)\n-        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n-        cos_theta_2 = torch.pow(cos_theta, 2)\n-        sin_theta_2 = 1 - cos_theta_2\n-        sin_theta = torch.sqrt(sin_theta_2)\n-        cos_theta_m = (cos_theta * self.cos_m - sin_theta * self.sin_m)\n-        # this condition controls the theta+m should in range [0, pi]\n-        #      0<=theta+m<=pi\n-        #     -m<=theta<=pi-m\n-        cond_v = cos_theta - self.threshold\n-        cond_mask = cond_v <= 0\n-        keep_val = (cos_theta - self.mm) # when theta not in [0,pi], use cosface instead\n-        cos_theta_m[cond_mask] = keep_val[cond_mask]\n-        output = cos_theta * 1.0 # a little bit hacky way to prevent in_place operation on cos_theta\n-        idx_ = torch.arange(0, nB, dtype=torch.long)\n-        output[idx_, label] = cos_theta_m[idx_, label]\n-        output *= self.s # scale up in order to make softmax work, first introduced in normface\n-        return output\n-\n-##################################  Cosface head #############################################################    \n-    \n-class Am_softmax(Module):\n-    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599    \n-    def __init__(self,embedding_size=512,classnum=51332):\n-        super(Am_softmax, self).__init__()\n-        self.classnum = classnum\n-        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n-        # initial kernel\n-        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n-        self.m = 0.35 # additive margin recommended by the paper\n-        self.s = 30. # see normface https://arxiv.org/abs/1704.06369\n-    def forward(self,embbedings,label):\n-        kernel_norm = l2_norm(self.kernel,axis=0)\n-        cos_theta = torch.mm(embbedings,kernel_norm)\n-        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n-        phi = cos_theta - self.m\n-        label = label.view(-1,1) #size=(B,1)\n-        index = cos_theta.data * 0.0 #size=(B,Classnum)\n-        index.scatter_(1,label.data.view(-1,1),1)\n-        index = index.byte()\n-        output = cos_theta * 1.0\n-        output[index] = phi[index] #only change the correct predicted output\n-        output *= self.s # scale up in order to make softmax work, first introduced in normface\n-        return output\n-\ndiff --git a/mtcnn.py b/mtcnn.py\ndeleted file mode 100644\nindex 90a72c2..0000000\n--- a/mtcnn.py\n+++ /dev/null\n@@ -1,151 +0,0 @@\n-import numpy as np\n-import torch\n-from PIL import Image\n-from torch.autograd import Variable\n-from mtcnn_pytorch.src.get_nets import PNet, RNet, ONet\n-from mtcnn_pytorch.src.box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\n-from mtcnn_pytorch.src.first_stage import run_first_stage\n-from mtcnn_pytorch.src.align_trans import get_reference_facial_points, warp_and_crop_face\n-device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\n-# device = \'cpu\'\n-\n-class MTCNN():\n-    def __init__(self):\n-        self.pnet = PNet().to(device)\n-        self.rnet = RNet().to(device)\n-        self.onet = ONet().to(device)\n-        self.pnet.eval()\n-        self.rnet.eval()\n-        self.onet.eval()\n-        self.refrence = get_reference_facial_points(default_square= True)\n-        \n-    def align(self, img):\n-        _, landmarks = self.detect_faces(img)\n-        facial5points = [[landmarks[0][j],landmarks[0][j+5]] for j in range(5)]\n-        warped_face = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112,112))\n-        return Image.fromarray(warped_face)\n-    \n-    def align_multi(self, img, limit=None, min_face_size=30.0):\n-        boxes, landmarks = self.detect_faces(img, min_face_size)\n-        if limit:\n-            boxes = boxes[:limit]\n-            landmarks = landmarks[:limit]\n-        faces = []\n-        for landmark in landmarks:\n-            facial5points = [[landmark[j],landmark[j+5]] for j in range(5)]\n-            warped_face = warp_and_crop_face(np.array(img), facial5points, self.refrence, crop_size=(112,112))\n-            faces.append(Image.fromarray(warped_face))\n-        return boxes, faces\n-\n-    def detect_faces(self, image, min_face_size=20.0,\n-                    thresholds=[0.9, 0.8, 0.8],\n-                    nms_thresholds=[0.9, 0.9, 0.8]):\n-        """\n-        Arguments:\n-            image: an instance of PIL.Image.\n-            min_face_size: a float number.\n-            thresholds: a list of length 3.\n-            nms_thresholds: a list of length 3.\n-\n-        Returns:\n-            two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n-            bounding boxes and facial landmarks.\n-        """\n-\n-        # BUILD AN IMAGE PYRAMID\n-        width, height = image.size\n-        min_length = min(height, width)\n-\n-        min_detection_size = 12\n-        factor = 0.707  # sqrt(0.5)\n-\n-        # scales for scaling the image\n-        scales = []\n-\n-        # scales the image so that\n-        # minimum size that we can detect equals to\n-        # minimum face size that we want to detect\n-        m = min_detection_size/min_face_size\n-        min_length *= m\n-\n-        factor_count = 0\n-        while min_length > min_detection_size:\n-            scales.append(m*factor**factor_count)\n-            min_length *= factor\n-            factor_count += 1\n-\n-        # STAGE 1\n-\n-        # it will be returned\n-        bounding_boxes = []\n-\n-        with torch.no_grad():\n-            # run P-Net on different scales\n-            for s in scales:\n-                boxes = run_first_stage(image, self.pnet, scale=s, threshold=thresholds[0])\n-                bounding_boxes.append(boxes)\n-\n-            # collect boxes (and offsets, and scores) from different scales\n-            bounding_boxes = [i for i in bounding_boxes if i is not None]\n-            bounding_boxes = np.vstack(bounding_boxes)\n-\n-            keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n-            bounding_boxes = bounding_boxes[keep]\n-\n-            # use offsets predicted by pnet to transform bounding boxes\n-            bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n-            # shape [n_boxes, 5]\n-\n-            bounding_boxes = convert_to_square(bounding_boxes)\n-            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n-\n-            # STAGE 2\n-\n-            img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n-            img_boxes = torch.FloatTensor(img_boxes).to(device)\n-\n-            output = self.rnet(img_boxes)\n-            offsets = output[0].cpu().data.numpy()  # shape [n_boxes, 4]\n-            probs = output[1].cpu().data.numpy()  # shape [n_boxes, 2]\n-\n-            keep = np.where(probs[:, 1] > thresholds[1])[0]\n-            bounding_boxes = bounding_boxes[keep]\n-            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n-            offsets = offsets[keep]\n-\n-            keep = nms(bounding_boxes, nms_thresholds[1])\n-            bounding_boxes = bounding_boxes[keep]\n-            bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n-            bounding_boxes = convert_to_square(bounding_boxes)\n-            bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n-\n-            # STAGE 3\n-\n-            img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n-            if len(img_boxes) == 0: \n-                return [], []\n-            img_boxes = torch.FloatTensor(img_boxes).to(device)\n-            output = self.onet(img_boxes)\n-            landmarks = output[0].cpu().data.numpy()  # shape [n_boxes, 10]\n-            offsets = output[1].cpu().data.numpy()  # shape [n_boxes, 4]\n-            probs = output[2].cpu().data.numpy()  # shape [n_boxes, 2]\n-\n-            keep = np.where(probs[:, 1] > thresholds[2])[0]\n-            bounding_boxes = bounding_boxes[keep]\n-            bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n-            offsets = offsets[keep]\n-            landmarks = landmarks[keep]\n-\n-            # compute landmark points\n-            width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n-            height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n-            xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n-            landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n-            landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n-\n-            bounding_boxes = calibrate_box(bounding_boxes, offsets)\n-            keep = nms(bounding_boxes, nms_thresholds[2], mode=\'min\')\n-            bounding_boxes = bounding_boxes[keep]\n-            landmarks = landmarks[keep]\n-\n-        return bounding_boxes, landmarks\ndiff --git a/mtcnn_pytorch/src/__init__.py b/mtcnn_pytorch/src/__init__.py\ndeleted file mode 100644\nindex 617ba38..0000000\n--- a/mtcnn_pytorch/src/__init__.py\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-from .visualization_utils import show_bboxes\n-from .detector import detect_faces\ndiff --git a/mtcnn_pytorch/src/align_trans.py b/mtcnn_pytorch/src/align_trans.py\ndeleted file mode 100644\nindex 72d4351..0000000\n--- a/mtcnn_pytorch/src/align_trans.py\n+++ /dev/null\n@@ -1,304 +0,0 @@\n-# -*- coding: utf-8 -*-\n-"""\n-Created on Mon Apr 24 15:43:29 2017\n-@author: zhaoy\n-"""\n-import numpy as np\n-import cv2\n-\n-# from scipy.linalg import lstsq\n-# from scipy.ndimage import geometric_transform  # , map_coordinates\n-\n-from mtcnn_pytorch.src.matlab_cp2tform import get_similarity_transform_for_cv2\n-\n-# reference facial points, a list of coordinates (x,y)\n-REFERENCE_FACIAL_POINTS = [\n-    [30.29459953,  51.69630051],\n-    [65.53179932,  51.50139999],\n-    [48.02519989,  71.73660278],\n-    [33.54930115,  92.3655014],\n-    [62.72990036,  92.20410156]\n-]\n-\n-DEFAULT_CROP_SIZE = (96, 112)\n-\n-\n-class FaceWarpException(Exception):\n-    def __str__(self):\n-        return \'In File {}:{}\'.format(\n-            __file__, super.__str__(self))\n-\n-\n-def get_reference_facial_points(output_size=None,\n-                                inner_padding_factor=0.0,\n-                                outer_padding=(0, 0),\n-                                default_square=False):\n-    """\n-    Function:\n-    ----------\n-        get reference 5 key points according to crop settings:\n-        0. Set default crop_size:\n-            if default_square: \n-                crop_size = (112, 112)\n-            else: \n-                crop_size = (96, 112)\n-        1. Pad the crop_size by inner_padding_factor in each side;\n-        2. Resize crop_size into (output_size - outer_padding*2),\n-            pad into output_size with outer_padding;\n-        3. Output reference_5point;\n-    Parameters:\n-    ----------\n-        @output_size: (w, h) or None\n-            size of aligned face image\n-        @inner_padding_factor: (w_factor, h_factor)\n-            padding factor for inner (w, h)\n-        @outer_padding: (w_pad, h_pad)\n-            each row is a pair of coordinates (x, y)\n-        @default_square: True or False\n-            if True:\n-                default crop_size = (112, 112)\n-            else:\n-                default crop_size = (96, 112);\n-        !!! make sure, if output_size is not None:\n-                (output_size - outer_padding) \n-                = some_scale * (default crop_size * (1.0 + inner_padding_factor))\n-    Returns:\n-    ----------\n-        @reference_5point: 5x2 np.array\n-            each row is a pair of transformed coordinates (x, y)\n-    """\n-    #print(\'\\n===> get_reference_facial_points():\')\n-\n-    #print(\'---> Params:\')\n-    #print(\'            output_size: \', output_size)\n-    #print(\'            inner_padding_factor: \', inner_padding_factor)\n-    #print(\'            outer_padding:\', outer_padding)\n-    #print(\'            default_square: \', default_square)\n-\n-    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\n-    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\n-\n-    # 0) make the inner region a square\n-    if default_square:\n-        size_diff = max(tmp_crop_size) - tmp_crop_size\n-        tmp_5pts += size_diff / 2\n-        tmp_crop_size += size_diff\n-\n-    #print(\'---> default:\')\n-    #print(\'              crop_size = \', tmp_crop_size)\n-    #print(\'              reference_5pts = \', tmp_5pts)\n-\n-    if (output_size and\n-            output_size[0] == tmp_crop_size[0] and\n-            output_size[1] == tmp_crop_size[1]):\n-        #print(\'output_size == DEFAULT_CROP_SIZE {}: return default reference points\'.format(tmp_crop_size))\n-        return tmp_5pts\n-\n-    if (inner_padding_factor == 0 and\n-            outer_padding == (0, 0)):\n-        if output_size is None:\n-            #print(\'No paddings to do: return default reference points\')\n-            return tmp_5pts\n-        else:\n-            raise FaceWarpException(\n-                \'No paddings to do, output_size must be None or {}\'.format(tmp_crop_size))\n-\n-    # check output size\n-    if not (0 <= inner_padding_factor <= 1.0):\n-        raise FaceWarpException(\'Not (0 <= inner_padding_factor <= 1.0)\')\n-\n-    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\n-            and output_size is None):\n-        output_size = tmp_crop_size * \\\n-            (1 + inner_padding_factor * 2).astype(np.int32)\n-        output_size += np.array(outer_padding)\n-        #print(\'              deduced from paddings, output_size = \', output_size)\n-\n-    if not (outer_padding[0] < output_size[0]\n-            and outer_padding[1] < output_size[1]):\n-        raise FaceWarpException(\'Not (outer_padding[0] < output_size[0]\'\n-                                \'and outer_padding[1] < output_size[1])\')\n-\n-    # 1) pad the inner region according inner_padding_factor\n-    #print(\'---> STEP1: pad the inner region according inner_padding_factor\')\n-    if inner_padding_factor > 0:\n-        size_diff = tmp_crop_size * inner_padding_factor * 2\n-        tmp_5pts += size_diff / 2\n-        tmp_crop_size += np.round(size_diff).astype(np.int32)\n-\n-    #print(\'              crop_size = \', tmp_crop_size)\n-    #print(\'              reference_5pts = \', tmp_5pts)\n-\n-    # 2) resize the padded inner region\n-    #print(\'---> STEP2: resize the padded inner region\')\n-    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\n-    #print(\'              crop_size = \', tmp_crop_size)\n-    #print(\'              size_bf_outer_pad = \', size_bf_outer_pad)\n-\n-    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\n-        raise FaceWarpException(\'Must have (output_size - outer_padding)\'\n-                                \'= some_scale * (crop_size * (1.0 + inner_padding_factor)\')\n-\n-    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\n-    #print(\'              resize scale_factor = \', scale_factor)\n-    tmp_5pts = tmp_5pts * scale_factor\n-#    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\n-#    tmp_5pts = tmp_5pts + size_diff / 2\n-    tmp_crop_size = size_bf_outer_pad\n-    #print(\'              crop_size = \', tmp_crop_size)\n-    #print(\'              reference_5pts = \', tmp_5pts)\n-\n-    # 3) add outer_padding to make output_size\n-    reference_5point = tmp_5pts + np.array(outer_padding)\n-    tmp_crop_size = output_size\n-    #print(\'---> STEP3: add outer_padding to make output_size\')\n-    #print(\'              crop_size = \', tmp_crop_size)\n-    #print(\'              reference_5pts = \', tmp_5pts)\n-\n-    #print(\'===> end get_reference_facial_points\\n\')\n-\n-    return reference_5point\n-\n-\n-def get_affine_transform_matrix(src_pts, dst_pts):\n-    """\n-    Function:\n-    ----------\n-        get affine transform matrix \'tfm\' from src_pts to dst_pts\n-    Parameters:\n-    ----------\n-        @src_pts: Kx2 np.array\n-            source points matrix, each row is a pair of coordinates (x, y)\n-        @dst_pts: Kx2 np.array\n-            destination points matrix, each row is a pair of coordinates (x, y)\n-    Returns:\n-    ----------\n-        @tfm: 2x3 np.array\n-            transform matrix from src_pts to dst_pts\n-    """\n-\n-    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\n-    n_pts = src_pts.shape[0]\n-    ones = np.ones((n_pts, 1), src_pts.dtype)\n-    src_pts_ = np.hstack([src_pts, ones])\n-    dst_pts_ = np.hstack([dst_pts, ones])\n-\n-#    #print((\'src_pts_:\\n\' + str(src_pts_))\n-#    #print((\'dst_pts_:\\n\' + str(dst_pts_))\n-\n-    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\n-\n-#    #print((\'np.linalg.lstsq return A: \\n\' + str(A))\n-#    #print((\'np.linalg.lstsq return res: \\n\' + str(res))\n-#    #print((\'np.linalg.lstsq return rank: \\n\' + str(rank))\n-#    #print((\'np.linalg.lstsq return s: \\n\' + str(s))\n-\n-    if rank == 3:\n-        tfm = np.float32([\n-            [A[0, 0], A[1, 0], A[2, 0]],\n-            [A[0, 1], A[1, 1], A[2, 1]]\n-        ])\n-    elif rank == 2:\n-        tfm = np.float32([\n-            [A[0, 0], A[1, 0], 0],\n-            [A[0, 1], A[1, 1], 0]\n-        ])\n-\n-    return tfm\n-\n-\n-def warp_and_crop_face(src_img,\n-                       facial_pts,\n-                       reference_pts=None,\n-                       crop_size=(96, 112),\n-                       align_type=\'smilarity\'):\n-    """\n-    Function:\n-    ----------\n-        apply affine transform \'trans\' to uv\n-    Parameters:\n-    ----------\n-        @src_img: 3x3 np.array\n-            input image\n-        @facial_pts: could be\n-            1)a list of K coordinates (x,y)\n-        or\n-            2) Kx2 or 2xK np.array\n-            each row or col is a pair of coordinates (x, y)\n-        @reference_pts: could be\n-            1) a list of K coordinates (x,y)\n-        or\n-            2) Kx2 or 2xK np.array\n-            each row or col is a pair of coordinates (x, y)\n-        or\n-            3) None\n-            if None, use default reference facial points\n-        @crop_size: (w, h)\n-            output face image size\n-        @align_type: transform type, could be one of\n-            1) \'similarity\': use similarity transform\n-            2) \'cv2_affine\': use the first 3 points to do affine transform,\n-                    by calling cv2.getAffineTransform()\n-            3) \'affine\': use all points to do affine transform\n-    Returns:\n-    ----------\n-        @face_img: output face image with size (w, h) = @crop_size\n-    """\n-\n-    if reference_pts is None:\n-        if crop_size[0] == 96 and crop_size[1] == 112:\n-            reference_pts = REFERENCE_FACIAL_POINTS\n-        else:\n-            default_square = False\n-            inner_padding_factor = 0\n-            outer_padding = (0, 0)\n-            output_size = crop_size\n-\n-            reference_pts = get_reference_facial_points(output_size,\n-                                                        inner_padding_factor,\n-                                                        outer_padding,\n-                                                        default_square)\n-\n-    ref_pts = np.float32(reference_pts)\n-    ref_pts_shp = ref_pts.shape\n-    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\n-        raise FaceWarpException(\n-            \'reference_pts.shape must be (K,2) or (2,K) and K>2\')\n-\n-    if ref_pts_shp[0] == 2:\n-        ref_pts = ref_pts.T\n-\n-    src_pts = np.float32(facial_pts)\n-    src_pts_shp = src_pts.shape\n-    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\n-        raise FaceWarpException(\n-            \'facial_pts.shape must be (K,2) or (2,K) and K>2\')\n-\n-    if src_pts_shp[0] == 2:\n-        src_pts = src_pts.T\n-\n-#    #print(\'--->src_pts:\\n\', src_pts\n-#    #print(\'--->ref_pts\\n\', ref_pts\n-\n-    if src_pts.shape != ref_pts.shape:\n-        raise FaceWarpException(\n-            \'facial_pts and reference_pts must have the same shape\')\n-\n-    if align_type is \'cv2_affine\':\n-        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\n-#        #print((\'cv2.getAffineTransform() returns tfm=\\n\' + str(tfm))\n-    elif align_type is \'affine\':\n-        tfm = get_affine_transform_matrix(src_pts, ref_pts)\n-#        #print((\'get_affine_transform_matrix() returns tfm=\\n\' + str(tfm))\n-    else:\n-        tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\n-#        #print((\'get_similarity_transform_for_cv2() returns tfm=\\n\' + str(tfm))\n-\n-#    #print(\'--->Transform matrix: \'\n-#    #print((\'type(tfm):\' + str(type(tfm)))\n-#    #print((\'tfm.dtype:\' + str(tfm.dtype))\n-#    #print( tfm\n-\n-    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\n-\n-    return face_img\n\\ No newline at end of file\ndiff --git a/mtcnn_pytorch/src/box_utils.py b/mtcnn_pytorch/src/box_utils.py\ndeleted file mode 100644\nindex 3557387..0000000\n--- a/mtcnn_pytorch/src/box_utils.py\n+++ /dev/null\n@@ -1,238 +0,0 @@\n-import numpy as np\n-from PIL import Image\n-\n-\n-def nms(boxes, overlap_threshold=0.5, mode=\'union\'):\n-    """Non-maximum suppression.\n-\n-    Arguments:\n-        boxes: a float numpy array of shape [n, 5],\n-            where each row is (xmin, ymin, xmax, ymax, score).\n-        overlap_threshold: a float number.\n-        mode: \'union\' or \'min\'.\n-\n-    Returns:\n-        list with indices of the selected boxes\n-    """\n-\n-    # if there are no boxes, return the empty list\n-    if len(boxes) == 0:\n-        return []\n-\n-    # list of picked indices\n-    pick = []\n-\n-    # grab the coordinates of the bounding boxes\n-    x1, y1, x2, y2, score = [boxes[:, i] for i in range(5)]\n-\n-    area = (x2 - x1 + 1.0)*(y2 - y1 + 1.0)\n-    ids = np.argsort(score)  # in increasing order\n-\n-    while len(ids) > 0:\n-\n-        # grab index of the largest value\n-        last = len(ids) - 1\n-        i = ids[last]\n-        pick.append(i)\n-\n-        # compute intersections\n-        # of the box with the largest score\n-        # with the rest of boxes\n-\n-        # left top corner of intersection boxes\n-        ix1 = np.maximum(x1[i], x1[ids[:last]])\n-        iy1 = np.maximum(y1[i], y1[ids[:last]])\n-\n-        # right bottom corner of intersection boxes\n-        ix2 = np.minimum(x2[i], x2[ids[:last]])\n-        iy2 = np.minimum(y2[i], y2[ids[:last]])\n-\n-        # width and height of intersection boxes\n-        w = np.maximum(0.0, ix2 - ix1 + 1.0)\n-        h = np.maximum(0.0, iy2 - iy1 + 1.0)\n-\n-        # intersections\' areas\n-        inter = w * h\n-        if mode == \'min\':\n-            overlap = inter/np.minimum(area[i], area[ids[:last]])\n-        elif mode == \'union\':\n-            # intersection over union (IoU)\n-            overlap = inter/(area[i] + area[ids[:last]] - inter)\n-\n-        # delete all boxes where overlap is too big\n-        ids = np.delete(\n-            ids,\n-            np.concatenate([[last], np.where(overlap > overlap_threshold)[0]])\n-        )\n-\n-    return pick\n-\n-\n-def convert_to_square(bboxes):\n-    """Convert bounding boxes to a square form.\n-\n-    Arguments:\n-        bboxes: a float numpy array of shape [n, 5].\n-\n-    Returns:\n-        a float numpy array of shape [n, 5],\n-            squared bounding boxes.\n-    """\n-\n-    square_bboxes = np.zeros_like(bboxes)\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n-    h = y2 - y1 + 1.0\n-    w = x2 - x1 + 1.0\n-    max_side = np.maximum(h, w)\n-    square_bboxes[:, 0] = x1 + w*0.5 - max_side*0.5\n-    square_bboxes[:, 1] = y1 + h*0.5 - max_side*0.5\n-    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n-    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n-    return square_bboxes\n-\n-\n-def calibrate_box(bboxes, offsets):\n-    """Transform bounding boxes to be more like true bounding boxes.\n-    \'offsets\' is one of the outputs of the nets.\n-\n-    Arguments:\n-        bboxes: a float numpy array of shape [n, 5].\n-        offsets: a float numpy array of shape [n, 4].\n-\n-    Returns:\n-        a float numpy array of shape [n, 5].\n-    """\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n-    w = x2 - x1 + 1.0\n-    h = y2 - y1 + 1.0\n-    w = np.expand_dims(w, 1)\n-    h = np.expand_dims(h, 1)\n-\n-    # this is what happening here:\n-    # tx1, ty1, tx2, ty2 = [offsets[:, i] for i in range(4)]\n-    # x1_true = x1 + tx1*w\n-    # y1_true = y1 + ty1*h\n-    # x2_true = x2 + tx2*w\n-    # y2_true = y2 + ty2*h\n-    # below is just more compact form of this\n-\n-    # are offsets always such that\n-    # x1 < x2 and y1 < y2 ?\n-\n-    translation = np.hstack([w, h, w, h])*offsets\n-    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\n-    return bboxes\n-\n-\n-def get_image_boxes(bounding_boxes, img, size=24):\n-    """Cut out boxes from the image.\n-\n-    Arguments:\n-        bounding_boxes: a float numpy array of shape [n, 5].\n-        img: an instance of PIL.Image.\n-        size: an integer, size of cutouts.\n-\n-    Returns:\n-        a float numpy array of shape [n, 3, size, size].\n-    """\n-\n-    num_boxes = len(bounding_boxes)\n-    width, height = img.size\n-\n-    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\n-    img_boxes = np.zeros((num_boxes, 3, size, size), \'float32\')\n-\n-    for i in range(num_boxes):\n-        img_box = np.zeros((h[i], w[i], 3), \'uint8\')\n-\n-        img_array = np.asarray(img, \'uint8\')\n-        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] =\\\n-            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\n-\n-        # resize\n-        img_box = Image.fromarray(img_box)\n-        img_box = img_box.resize((size, size), Image.BILINEAR)\n-        img_box = np.asarray(img_box, \'float32\')\n-\n-        img_boxes[i, :, :, :] = _preprocess(img_box)\n-\n-    return img_boxes\n-\n-\n-def correct_bboxes(bboxes, width, height):\n-    """Crop boxes that are too big and get coordinates\n-    with respect to cutouts.\n-\n-    Arguments:\n-        bboxes: a float numpy array of shape [n, 5],\n-            where each row is (xmin, ymin, xmax, ymax, score).\n-        width: a float number.\n-        height: a float number.\n-\n-    Returns:\n-        dy, dx, edy, edx: a int numpy arrays of shape [n],\n-            coordinates of the boxes with respect to the cutouts.\n-        y, x, ey, ex: a int numpy arrays of shape [n],\n-            corrected ymin, xmin, ymax, xmax.\n-        h, w: a int numpy arrays of shape [n],\n-            just heights and widths of boxes.\n-\n-        in the following order:\n-            [dy, edy, dx, edx, y, ey, x, ex, w, h].\n-    """\n-\n-    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n-    w, h = x2 - x1 + 1.0,  y2 - y1 + 1.0\n-    num_boxes = bboxes.shape[0]\n-\n-    # \'e\' stands for end\n-    # (x, y) -> (ex, ey)\n-    x, y, ex, ey = x1, y1, x2, y2\n-\n-    # we need to cut out a box from the image.\n-    # (x, y, ex, ey) are corrected coordinates of the box\n-    # in the image.\n-    # (dx, dy, edx, edy) are coordinates of the box in the cutout\n-    # from the image.\n-    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\n-    edx, edy = w.copy() - 1.0, h.copy() - 1.0\n-\n-    # if box\'s bottom right corner is too far right\n-    ind = np.where(ex > width - 1.0)[0]\n-    edx[ind] = w[ind] + width - 2.0 - ex[ind]\n-    ex[ind] = width - 1.0\n-\n-    # if box\'s bottom right corner is too low\n-    ind = np.where(ey > height - 1.0)[0]\n-    edy[ind] = h[ind] + height - 2.0 - ey[ind]\n-    ey[ind] = height - 1.0\n-\n-    # if box\'s top left corner is too far left\n-    ind = np.where(x < 0.0)[0]\n-    dx[ind] = 0.0 - x[ind]\n-    x[ind] = 0.0\n-\n-    # if box\'s top left corner is too high\n-    ind = np.where(y < 0.0)[0]\n-    dy[ind] = 0.0 - y[ind]\n-    y[ind] = 0.0\n-\n-    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\n-    return_list = [i.astype(\'int32\') for i in return_list]\n-\n-    return return_list\n-\n-\n-def _preprocess(img):\n-    """Preprocessing step before feeding the network.\n-\n-    Arguments:\n-        img: a float numpy array of shape [h, w, c].\n-\n-    Returns:\n-        a float numpy array of shape [1, c, h, w].\n-    """\n-    img = img.transpose((2, 0, 1))\n-    img = np.expand_dims(img, 0)\n-    img = (img - 127.5)*0.0078125\n-    return img\ndiff --git a/mtcnn_pytorch/src/detector.py b/mtcnn_pytorch/src/detector.py\ndeleted file mode 100644\nindex f66eaa5..0000000\n--- a/mtcnn_pytorch/src/detector.py\n+++ /dev/null\n@@ -1,126 +0,0 @@\n-import numpy as np\n-import torch\n-from torch.autograd import Variable\n-from .get_nets import PNet, RNet, ONet\n-from .box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\n-from .first_stage import run_first_stage\n-\n-\n-def detect_faces(image, min_face_size=20.0,\n-                 thresholds=[0.6, 0.7, 0.8],\n-                 nms_thresholds=[0.7, 0.7, 0.7]):\n-    """\n-    Arguments:\n-        image: an instance of PIL.Image.\n-        min_face_size: a float number.\n-        thresholds: a list of length 3.\n-        nms_thresholds: a list of length 3.\n-\n-    Returns:\n-        two float numpy arrays of shapes [n_boxes, 4] and [n_boxes, 10],\n-        bounding boxes and facial landmarks.\n-    """\n-\n-    # LOAD MODELS\n-    pnet = PNet()\n-    rnet = RNet()\n-    onet = ONet()\n-    onet.eval()\n-\n-    # BUILD AN IMAGE PYRAMID\n-    width, height = image.size\n-    min_length = min(height, width)\n-\n-    min_detection_size = 12\n-    factor = 0.707  # sqrt(0.5)\n-\n-    # scales for scaling the image\n-    scales = []\n-\n-    # scales the image so that\n-    # minimum size that we can detect equals to\n-    # minimum face size that we want to detect\n-    m = min_detection_size/min_face_size\n-    min_length *= m\n-\n-    factor_count = 0\n-    while min_length > min_detection_size:\n-        scales.append(m*factor**factor_count)\n-        min_length *= factor\n-        factor_count += 1\n-\n-    # STAGE 1\n-\n-    # it will be returned\n-    bounding_boxes = []\n-    \n-    with torch.no_grad():\n-        # run P-Net on different scales\n-        for s in scales:\n-            boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n-            bounding_boxes.append(boxes)\n-\n-        # collect boxes (and offsets, and scores) from different scales\n-        bounding_boxes = [i for i in bounding_boxes if i is not None]\n-        bounding_boxes = np.vstack(bounding_boxes)\n-\n-        keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n-        bounding_boxes = bounding_boxes[keep]\n-\n-        # use offsets predicted by pnet to transform bounding boxes\n-        bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n-        # shape [n_boxes, 5]\n-\n-        bounding_boxes = convert_to_square(bounding_boxes)\n-        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n-\n-        # STAGE 2\n-\n-        img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n-        img_boxes = torch.FloatTensor(img_boxes)\n-\n-        output = rnet(img_boxes)\n-        offsets = output[0].data.numpy()  # shape [n_boxes, 4]\n-        probs = output[1].data.numpy()  # shape [n_boxes, 2]\n-\n-        keep = np.where(probs[:, 1] > thresholds[1])[0]\n-        bounding_boxes = bounding_boxes[keep]\n-        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n-        offsets = offsets[keep]\n-\n-        keep = nms(bounding_boxes, nms_thresholds[1])\n-        bounding_boxes = bounding_boxes[keep]\n-        bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n-        bounding_boxes = convert_to_square(bounding_boxes)\n-        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n-\n-        # STAGE 3\n-\n-        img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n-        if len(img_boxes) == 0: \n-            return [], []\n-        img_boxes = torch.FloatTensor(img_boxes)\n-        output = onet(img_boxes)\n-        landmarks = output[0].data.numpy()  # shape [n_boxes, 10]\n-        offsets = output[1].data.numpy()  # shape [n_boxes, 4]\n-        probs = output[2].data.numpy()  # shape [n_boxes, 2]\n-\n-        keep = np.where(probs[:, 1] > thresholds[2])[0]\n-        bounding_boxes = bounding_boxes[keep]\n-        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n-        offsets = offsets[keep]\n-        landmarks = landmarks[keep]\n-\n-        # compute landmark points\n-        width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n-        height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n-        xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n-        landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n-        landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n-\n-        bounding_boxes = calibrate_box(bounding_boxes, offsets)\n-        keep = nms(bounding_boxes, nms_thresholds[2], mode=\'min\')\n-        bounding_boxes = bounding_boxes[keep]\n-        landmarks = landmarks[keep]\n-\n-    return bounding_boxes, landmarks\ndiff --git a/mtcnn_pytorch/src/first_stage.py b/mtcnn_pytorch/src/first_stage.py\ndeleted file mode 100644\nindex 55ed04a..0000000\n--- a/mtcnn_pytorch/src/first_stage.py\n+++ /dev/null\n@@ -1,99 +0,0 @@\n-import torch\n-from torch.autograd import Variable\n-import math\n-from PIL import Image\n-import numpy as np\n-from .box_utils import nms, _preprocess\n-device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\n-# device = \'cpu\'\n-\n-def run_first_stage(image, net, scale, threshold):\n-    """Run P-Net, generate bounding boxes, and do NMS.\n-\n-    Arguments:\n-        image: an instance of PIL.Image.\n-        net: an instance of pytorch\'s nn.Module, P-Net.\n-        scale: a float number,\n-            scale width and height of the image by this number.\n-        threshold: a float number,\n-            threshold on the probability of a face when generating\n-            bounding boxes from predictions of the net.\n-\n-    Returns:\n-        a float numpy array of shape [n_boxes, 9],\n-            bounding boxes with scores and offsets (4 + 1 + 4).\n-    """\n-\n-    # scale the image and convert it to a float array\n-    width, height = image.size\n-    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n-    img = image.resize((sw, sh), Image.BILINEAR)\n-    img = np.asarray(img, \'float32\')\n-\n-    img = torch.FloatTensor(_preprocess(img)).to(device)\n-    with torch.no_grad():\n-        output = net(img)\n-        probs = output[1].cpu().data.numpy()[0, 1, :, :]\n-        offsets = output[0].cpu().data.numpy()\n-        # probs: probability of a face at each sliding window\n-        # offsets: transformations to true bounding boxes\n-\n-        boxes = _generate_bboxes(probs, offsets, scale, threshold)\n-        if len(boxes) == 0:\n-            return None\n-\n-        keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n-    return boxes[keep]\n-\n-\n-def _generate_bboxes(probs, offsets, scale, threshold):\n-    """Generate bounding boxes at places\n-    where there is probably a face.\n-\n-    Arguments:\n-        probs: a float numpy array of shape [n, m].\n-        offsets: a float numpy array of shape [1, 4, n, m].\n-        scale: a float number,\n-            width and height of the image were scaled by this number.\n-        threshold: a float number.\n-\n-    Returns:\n-        a float numpy array of shape [n_boxes, 9]\n-    """\n-\n-    # applying P-Net is equivalent, in some sense, to\n-    # moving 12x12 window with stride 2\n-    stride = 2\n-    cell_size = 12\n-\n-    # indices of boxes where there is probably a face\n-    inds = np.where(probs > threshold)\n-\n-    if inds[0].size == 0:\n-        return np.array([])\n-\n-    # transformations of bounding boxes\n-    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n-    # they are defined as:\n-    # w = x2 - x1 + 1\n-    # h = y2 - y1 + 1\n-    # x1_true = x1 + tx1*w\n-    # x2_true = x2 + tx2*w\n-    # y1_true = y1 + ty1*h\n-    # y2_true = y2 + ty2*h\n-\n-    offsets = np.array([tx1, ty1, tx2, ty2])\n-    score = probs[inds[0], inds[1]]\n-\n-    # P-Net is applied to scaled images\n-    # so we need to rescale bounding boxes back\n-    bounding_boxes = np.vstack([\n-        np.round((stride*inds[1] + 1.0)/scale),\n-        np.round((stride*inds[0] + 1.0)/scale),\n-        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n-        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n-        score, offsets\n-    ])\n-    # why one is added?\n-\n-    return bounding_boxes.T\ndiff --git a/mtcnn_pytorch/src/get_nets.py b/mtcnn_pytorch/src/get_nets.py\ndeleted file mode 100644\nindex 6df979a..0000000\n--- a/mtcnn_pytorch/src/get_nets.py\n+++ /dev/null\n@@ -1,169 +0,0 @@\n-import torch\n-import torch.nn as nn\n-import torch.nn.functional as F\n-from collections import OrderedDict\n-import numpy as np\n-\n-\n-class Flatten(nn.Module):\n-\n-    def __init__(self):\n-        super(Flatten, self).__init__()\n-\n-    def forward(self, x):\n-        """\n-        Arguments:\n-            x: a float tensor with shape [batch_size, c, h, w].\n-        Returns:\n-            a float tensor with shape [batch_size, c*h*w].\n-        """\n-\n-        # without this pretrained model isn\'t working\n-        x = x.transpose(3, 2).contiguous()\n-\n-        return x.view(x.size(0), -1)\n-\n-\n-class PNet(nn.Module):\n-\n-    def __init__(self):\n-\n-        super(PNet, self).__init__()\n-\n-        # suppose we have input with size HxW, then\n-        # after first layer: H - 2,\n-        # after pool: ceil((H - 2)/2),\n-        # after second conv: ceil((H - 2)/2) - 2,\n-        # after last conv: ceil((H - 2)/2) - 4,\n-        # and the same for W\n-\n-        self.features = nn.Sequential(OrderedDict([\n-            (\'conv1\', nn.Conv2d(3, 10, 3, 1)),\n-            (\'prelu1\', nn.PReLU(10)),\n-            (\'pool1\', nn.MaxPool2d(2, 2, ceil_mode=True)),\n-\n-            (\'conv2\', nn.Conv2d(10, 16, 3, 1)),\n-            (\'prelu2\', nn.PReLU(16)),\n-\n-            (\'conv3\', nn.Conv2d(16, 32, 3, 1)),\n-            (\'prelu3\', nn.PReLU(32))\n-        ]))\n-\n-        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n-        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n-\n-        weights = np.load(\'mtcnn_pytorch/src/weights/pnet.npy\')[()]\n-        for n, p in self.named_parameters():\n-            p.data = torch.FloatTensor(weights[n])\n-\n-    def forward(self, x):\n-        """\n-        Arguments:\n-            x: a float tensor with shape [batch_size, 3, h, w].\n-        Returns:\n-            b: a float tensor with shape [batch_size, 4, h\', w\'].\n-            a: a float tensor with shape [batch_size, 2, h\', w\'].\n-        """\n-        x = self.features(x)\n-        a = self.conv4_1(x)\n-        b = self.conv4_2(x)\n-        a = F.softmax(a, dim= 1)\n-        return b, a\n-\n-\n-class RNet(nn.Module):\n-\n-    def __init__(self):\n-\n-        super(RNet, self).__init__()\n-\n-        self.features = nn.Sequential(OrderedDict([\n-            (\'conv1\', nn.Conv2d(3, 28, 3, 1)),\n-            (\'prelu1\', nn.PReLU(28)),\n-            (\'pool1\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n-\n-            (\'conv2\', nn.Conv2d(28, 48, 3, 1)),\n-            (\'prelu2\', nn.PReLU(48)),\n-            (\'pool2\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n-\n-            (\'conv3\', nn.Conv2d(48, 64, 2, 1)),\n-            (\'prelu3\', nn.PReLU(64)),\n-\n-            (\'flatten\', Flatten()),\n-            (\'conv4\', nn.Linear(576, 128)),\n-            (\'prelu4\', nn.PReLU(128))\n-        ]))\n-\n-        self.conv5_1 = nn.Linear(128, 2)\n-        self.conv5_2 = nn.Linear(128, 4)\n-\n-        weights = np.load(\'mtcnn_pytorch/src/weights/rnet.npy\')[()]\n-        for n, p in self.named_parameters():\n-            p.data = torch.FloatTensor(weights[n])\n-\n-    def forward(self, x):\n-        """\n-        Arguments:\n-            x: a float tensor with shape [batch_size, 3, h, w].\n-        Returns:\n-            b: a float tensor with shape [batch_size, 4].\n-            a: a float tensor with shape [batch_size, 2].\n-        """\n-        x = self.features(x)\n-        a = self.conv5_1(x)\n-        b = self.conv5_2(x)\n-        a = F.softmax(a, dim=-1)\n-        return b, a\n-\n-\n-class ONet(nn.Module):\n-\n-    def __init__(self):\n-\n-        super(ONet, self).__init__()\n-\n-        self.features = nn.Sequential(OrderedDict([\n-            (\'conv1\', nn.Conv2d(3, 32, 3, 1)),\n-            (\'prelu1\', nn.PReLU(32)),\n-            (\'pool1\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n-\n-            (\'conv2\', nn.Conv2d(32, 64, 3, 1)),\n-            (\'prelu2\', nn.PReLU(64)),\n-            (\'pool2\', nn.MaxPool2d(3, 2, ceil_mode=True)),\n-\n-            (\'conv3\', nn.Conv2d(64, 64, 3, 1)),\n-            (\'prelu3\', nn.PReLU(64)),\n-            (\'pool3\', nn.MaxPool2d(2, 2, ceil_mode=True)),\n-\n-            (\'conv4\', nn.Conv2d(64, 128, 2, 1)),\n-            (\'prelu4\', nn.PReLU(128)),\n-\n-            (\'flatten\', Flatten()),\n-            (\'conv5\', nn.Linear(1152, 256)),\n-            (\'drop5\', nn.Dropout(0.25)),\n-            (\'prelu5\', nn.PReLU(256)),\n-        ]))\n-\n-        self.conv6_1 = nn.Linear(256, 2)\n-        self.conv6_2 = nn.Linear(256, 4)\n-        self.conv6_3 = nn.Linear(256, 10)\n-\n-        weights = np.load(\'mtcnn_pytorch/src/weights/onet.npy\')[()]\n-        for n, p in self.named_parameters():\n-            p.data = torch.FloatTensor(weights[n])\n-\n-    def forward(self, x):\n-        """\n-        Arguments:\n-            x: a float tensor with shape [batch_size, 3, h, w].\n-        Returns:\n-            c: a float tensor with shape [batch_size, 10].\n-            b: a float tensor with shape [batch_size, 4].\n-            a: a float tensor with shape [batch_size, 2].\n-        """\n-        x = self.features(x)\n-        a = self.conv6_1(x)\n-        b = self.conv6_2(x)\n-        c = self.conv6_3(x)\n-        a = F.softmax(a, dim = -1)\n-        return c, b, a\ndiff --git a/mtcnn_pytorch/src/matlab_cp2tform.py b/mtcnn_pytorch/src/matlab_cp2tform.py\ndeleted file mode 100644\nindex cdcdf96..0000000\n--- a/mtcnn_pytorch/src/matlab_cp2tform.py\n+++ /dev/null\n@@ -1,350 +0,0 @@\n-# -*- coding: utf-8 -*-\n-"""\n-Created on Tue Jul 11 06:54:28 2017\n-\n-@author: zhaoyafei\n-"""\n-\n-import numpy as np\n-from numpy.linalg import inv, norm, lstsq\n-from numpy.linalg import matrix_rank as rank\n-\n-class MatlabCp2tormException(Exception):\n-    def __str__(self):\n-        return \'In File {}:{}\'.format(\n-                __file__, super.__str__(self))\n-\n-def tformfwd(trans, uv):\n-    """\n-    Function:\n-    ----------\n-        apply affine transform \'trans\' to uv\n-\n-    Parameters:\n-    ----------\n-        @trans: 3x3 np.array\n-            transform matrix\n-        @uv: Kx2 np.array\n-            each row is a pair of coordinates (x, y)\n-\n-    Returns:\n-    ----------\n-        @xy: Kx2 np.array\n-            each row is a pair of transformed coordinates (x, y)\n-    """\n-    uv = np.hstack((\n-        uv, np.ones((uv.shape[0], 1))\n-    ))\n-    xy = np.dot(uv, trans)\n-    xy = xy[:, 0:-1]\n-    return xy\n-\n-\n-def tforminv(trans, uv):\n-    """\n-    Function:\n-    ----------\n-        apply the inverse of affine transform \'trans\' to uv\n-\n-    Parameters:\n-    ----------\n-        @trans: 3x3 np.array\n-            transform matrix\n-        @uv: Kx2 np.array\n-            each row is a pair of coordinates (x, y)\n-\n-    Returns:\n-    ----------\n-        @xy: Kx2 np.array\n-            each row is a pair of inverse-transformed coordinates (x, y)\n-    """\n-    Tinv = inv(trans)\n-    xy = tformfwd(Tinv, uv)\n-    return xy\n-\n-\n-def findNonreflectiveSimilarity(uv, xy, options=None):\n-\n-    options = {\'K\': 2}\n-\n-    K = options[\'K\']\n-    M = xy.shape[0]\n-    x = xy[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n-    y = xy[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n-    # print(\'--->x, y:\\n\', x, y\n-\n-    tmp1 = np.hstack((x, y, np.ones((M, 1)), np.zeros((M, 1))))\n-    tmp2 = np.hstack((y, -x, np.zeros((M, 1)), np.ones((M, 1))))\n-    X = np.vstack((tmp1, tmp2))\n-    # print(\'--->X.shape: \', X.shape\n-    # print(\'X:\\n\', X\n-\n-    u = uv[:, 0].reshape((-1, 1))  # use reshape to keep a column vector\n-    v = uv[:, 1].reshape((-1, 1))  # use reshape to keep a column vector\n-    U = np.vstack((u, v))\n-    # print(\'--->U.shape: \', U.shape\n-    # print(\'U:\\n\', U\n-\n-    # We know that X * r = U\n-    if rank(X) >= 2 * K:\n-        r, _, _, _ = lstsq(X, U)\n-        r = np.squeeze(r)\n-    else:\n-        raise Exception(\'cp2tform:twoUniquePointsReq\')\n-\n-    # print(\'--->r:\\n\', r\n-\n-    sc = r[0]\n-    ss = r[1]\n-    tx = r[2]\n-    ty = r[3]\n-\n-    Tinv = np.array([\n-        [sc, -ss, 0],\n-        [ss,  sc, 0],\n-        [tx,  ty, 1]\n-    ])\n-\n-    # print(\'--->Tinv:\\n\', Tinv\n-\n-    T = inv(Tinv)\n-    # print(\'--->T:\\n\', T\n-\n-    T[:, 2] = np.array([0, 0, 1])\n-\n-    return T, Tinv\n-\n-\n-def findSimilarity(uv, xy, options=None):\n-\n-    options = {\'K\': 2}\n-\n-#    uv = np.array(uv)\n-#    xy = np.array(xy)\n-\n-    # Solve for trans1\n-    trans1, trans1_inv = findNonreflectiveSimilarity(uv, xy, options)\n-\n-    # Solve for trans2\n-\n-    # manually reflect the xy data across the Y-axis\n-    xyR = xy\n-    xyR[:, 0] = -1 * xyR[:, 0]\n-\n-    trans2r, trans2r_inv = findNonreflectiveSimilarity(uv, xyR, options)\n-\n-    # manually reflect the tform to undo the reflection done on xyR\n-    TreflectY = np.array([\n-        [-1, 0, 0],\n-        [0, 1, 0],\n-        [0, 0, 1]\n-    ])\n-\n-    trans2 = np.dot(trans2r, TreflectY)\n-\n-    # Figure out if trans1 or trans2 is better\n-    xy1 = tformfwd(trans1, uv)\n-    norm1 = norm(xy1 - xy)\n-\n-    xy2 = tformfwd(trans2, uv)\n-    norm2 = norm(xy2 - xy)\n-\n-    if norm1 <= norm2:\n-        return trans1, trans1_inv\n-    else:\n-        trans2_inv = inv(trans2)\n-        return trans2, trans2_inv\n-\n-\n-def get_similarity_transform(src_pts, dst_pts, reflective=True):\n-    """\n-    Function:\n-    ----------\n-        Find Similarity Transform Matrix \'trans\':\n-            u = src_pts[:, 0]\n-            v = src_pts[:, 1]\n-            x = dst_pts[:, 0]\n-            y = dst_pts[:, 1]\n-            [x, y, 1] = [u, v, 1] * trans\n-\n-    Parameters:\n-    ----------\n-        @src_pts: Kx2 np.array\n-            source points, each row is a pair of coordinates (x, y)\n-        @dst_pts: Kx2 np.array\n-            destination points, each row is a pair of transformed\n-            coordinates (x, y)\n-        @reflective: True or False\n-            if True:\n-                use reflective similarity transform\n-            else:\n-                use non-reflective similarity transform\n-\n-    Returns:\n-    ----------\n-       @trans: 3x3 np.array\n-            transform matrix from uv to xy\n-        trans_inv: 3x3 np.array\n-            inverse of trans, transform matrix from xy to uv\n-    """\n-\n-    if reflective:\n-        trans, trans_inv = findSimilarity(src_pts, dst_pts)\n-    else:\n-        trans, trans_inv = findNonreflectiveSimilarity(src_pts, dst_pts)\n-\n-    return trans, trans_inv\n-\n-\n-def cvt_tform_mat_for_cv2(trans):\n-    """\n-    Function:\n-    ----------\n-        Convert Transform Matrix \'trans\' into \'cv2_trans\' which could be\n-        directly used by cv2.warpAffine():\n-            u = src_pts[:, 0]\n-            v = src_pts[:, 1]\n-            x = dst_pts[:, 0]\n-            y = dst_pts[:, 1]\n-            [x, y].T = cv_trans * [u, v, 1].T\n-\n-    Parameters:\n-    ----------\n-        @trans: 3x3 np.array\n-            transform matrix from uv to xy\n-\n-    Returns:\n-    ----------\n-        @cv2_trans: 2x3 np.array\n-            transform matrix from src_pts to dst_pts, could be directly used\n-            for cv2.warpAffine()\n-    """\n-    cv2_trans = trans[:, 0:2].T\n-\n-    return cv2_trans\n-\n-\n-def get_similarity_transform_for_cv2(src_pts, dst_pts, reflective=True):\n-    """\n-    Function:\n-    ----------\n-        Find Similarity Transform Matrix \'cv2_trans\' which could be\n-        directly used by cv2.warpAffine():\n-            u = src_pts[:, 0]\n-            v = src_pts[:, 1]\n-            x = dst_pts[:, 0]\n-            y = dst_pts[:, 1]\n-            [x, y].T = cv_trans * [u, v, 1].T\n-\n-    Parameters:\n-    ----------\n-        @src_pts: Kx2 np.array\n-            source points, each row is a pair of coordinates (x, y)\n-        @dst_pts: Kx2 np.array\n-            destination points, each row is a pair of transformed\n-            coordinates (x, y)\n-        reflective: True or False\n-            if True:\n-                use reflective similarity transform\n-            else:\n-                use non-reflective similarity transform\n-\n-    Returns:\n-    ----------\n-        @cv2_trans: 2x3 np.array\n-            transform matrix from src_pts to dst_pts, could be directly used\n-            for cv2.warpAffine()\n-    """\n-    trans, trans_inv = get_similarity_transform(src_pts, dst_pts, reflective)\n-    cv2_trans = cvt_tform_mat_for_cv2(trans)\n-\n-    return cv2_trans\n-\n-\n-if __name__ == \'__main__\':\n-    """\n-    u = [0, 6, -2]\n-    v = [0, 3, 5]\n-    x = [-1, 0, 4]\n-    y = [-1, -10, 4]\n-\n-    # In Matlab, run:\n-    #\n-    #   uv = [u\'; v\'];\n-    #   xy = [x\'; y\'];\n-    #   tform_sim=cp2tform(uv,xy,\'similarity\');\n-    #\n-    #   trans = tform_sim.tdata.T\n-    #   ans =\n-    #       -0.0764   -1.6190         0\n-    #        1.6190   -0.0764         0\n-    #       -3.2156    0.0290    1.0000\n-    #   trans_inv = tform_sim.tdata.Tinv\n-    #    ans =\n-    #\n-    #       -0.0291    0.6163         0\n-    #       -0.6163   -0.0291         0\n-    #       -0.0756    1.9826    1.0000\n-    #    xy_m=tformfwd(tform_sim, u,v)\n-    #\n-    #    xy_m =\n-    #\n-    #       -3.2156    0.0290\n-    #        1.1833   -9.9143\n-    #        5.0323    2.8853\n-    #    uv_m=tforminv(tform_sim, x,y)\n-    #\n-    #    uv_m =\n-    #\n-    #        0.5698    1.3953\n-    #        6.0872    2.2733\n-    #       -2.6570    4.3314\n-    """\n-    u = [0, 6, -2]\n-    v = [0, 3, 5]\n-    x = [-1, 0, 4]\n-    y = [-1, -10, 4]\n-\n-    uv = np.array((u, v)).T\n-    xy = np.array((x, y)).T\n-\n-    print(\'\\n--->uv:\')\n-    print(uv)\n-    print(\'\\n--->xy:\')\n-    print(xy)\n-\n-    trans, trans_inv = get_similarity_transform(uv, xy)\n-\n-    print(\'\\n--->trans matrix:\')\n-    print(trans)\n-\n-    print(\'\\n--->trans_inv matrix:\')\n-    print(trans_inv)\n-\n-    print(\'\\n---> apply transform to uv\')\n-    print(\'\\nxy_m = uv_augmented * trans\')\n-    uv_aug = np.hstack((\n-        uv, np.ones((uv.shape[0], 1))\n-    ))\n-    xy_m = np.dot(uv_aug, trans)\n-    print(xy_m)\n-\n-    print(\'\\nxy_m = tformfwd(trans, uv)\')\n-    xy_m = tformfwd(trans, uv)\n-    print(xy_m)\n-\n-    print(\'\\n---> apply inverse transform to xy\')\n-    print(\'\\nuv_m = xy_augmented * trans_inv\')\n-    xy_aug = np.hstack((\n-        xy, np.ones((xy.shape[0], 1))\n-    ))\n-    uv_m = np.dot(xy_aug, trans_inv)\n-    print(uv_m)\n-\n-    print(\'\\nuv_m = tformfwd(trans_inv, xy)\')\n-    uv_m = tformfwd(trans_inv, xy)\n-    print(uv_m)\n-\n-    uv_m = tforminv(trans, xy)\n-    print(\'\\nuv_m = tforminv(trans, xy)\')\n-    print(uv_m)\ndiff --git a/mtcnn_pytorch/src/visualization_utils.py b/mtcnn_pytorch/src/visualization_utils.py\ndeleted file mode 100644\nindex bab02be..0000000\n--- a/mtcnn_pytorch/src/visualization_utils.py\n+++ /dev/null\n@@ -1,31 +0,0 @@\n-from PIL import ImageDraw\n-\n-\n-def show_bboxes(img, bounding_boxes, facial_landmarks=[]):\n-    """Draw bounding boxes and facial landmarks.\n-\n-    Arguments:\n-        img: an instance of PIL.Image.\n-        bounding_boxes: a float numpy array of shape [n, 5].\n-        facial_landmarks: a float numpy array of shape [n, 10].\n-\n-    Returns:\n-        an instance of PIL.Image.\n-    """\n-\n-    img_copy = img.copy()\n-    draw = ImageDraw.Draw(img_copy)\n-\n-    for b in bounding_boxes:\n-        draw.rectangle([\n-            (b[0], b[1]), (b[2], b[3])\n-        ], outline=\'white\')\n-\n-    for p in facial_landmarks:\n-        for i in range(5):\n-            draw.ellipse([\n-                (p[i] - 1.0, p[i + 5] - 1.0),\n-                (p[i] + 1.0, p[i + 5] + 1.0)\n-            ], outline=\'blue\')\n-\n-    return img_copy\ndiff --git a/mtcnn_pytorch/src/weights/onet.npy b/mtcnn_pytorch/src/weights/onet.npy\ndeleted file mode 100644\nindex e8f63e5..0000000\nBinary files a/mtcnn_pytorch/src/weights/onet.npy and /dev/null differ\ndiff --git a/mtcnn_pytorch/src/weights/pnet.npy b/mtcnn_pytorch/src/weights/pnet.npy\ndeleted file mode 100644\nindex 91f8f9c..0000000\nBinary files a/mtcnn_pytorch/src/weights/pnet.npy and /dev/null differ\ndiff --git a/mtcnn_pytorch/src/weights/rnet.npy b/mtcnn_pytorch/src/weights/rnet.npy\ndeleted file mode 100644\nindex 5e9bbab..0000000\nBinary files a/mtcnn_pytorch/src/weights/rnet.npy and /dev/null differ\ndiff --git a/prepare_data.py b/prepare_data.py\ndeleted file mode 100644\nindex 91ab673..0000000\n--- a/prepare_data.py\n+++ /dev/null\n@@ -1,17 +0,0 @@\n-from pathlib import Path\n-from config import get_config\n-from data.data_pipe import load_bin, load_mx_rec\n-import argparse\n-\n-if __name__ == \'__main__\':\n-    parser = argparse.ArgumentParser(description=\'for face verification\')\n-    parser.add_argument("-r", "--rec_path", help="mxnet record file path",default=\'faces_emore\', type=str)\n-    args = parser.parse_args()\n-    conf = get_config()\n-    rec_path = conf.data_path/args.rec_path\n-    load_mx_rec(rec_path)\n-    \n-    bin_files = [\'agedb_30\', \'cfp_fp\', \'lfw\', \'calfw\', \'cfp_ff\', \'cplfw\', \'vgg2_fp\']\n-    \n-    for i in range(len(bin_files)):\n-        load_bin(rec_path/(bin_files[i]+\'.bin\'), rec_path/bin_files[i], conf.test_transform)\n\\ No newline at end of file\ndiff --git a/requirements.txt b/requirements.txt\ndeleted file mode 100644\nindex fe4dc43..0000000\n--- a/requirements.txt\n+++ /dev/null\n@@ -1,19 +0,0 @@\n-Flask==1.1.1\n-scipy==1.1.0\n-matplotlib\n-Werkzeug==0.15.5\n-easydict==1.9\n-opencv_python==3.4.1.15\n-gevent==1.4.0\n-mxnet_mkl==1.6.0b20190914\n-tqdm==4.35.0\n-bcolz==1.2.1\n-Pillow==6.1.0\n-mtcnn_pytorch==1.0.2\n-mxnet==1.5.0\n-scikit_learn==0.21.3\n-tensorboardX==1.8\n-torch\n-torchvision\n-numpy==1.16.1\n-\ndiff --git a/templates/index.html b/templates/index.html\ndeleted file mode 100644\nindex b54f90b..0000000\n--- a/templates/index.html\n+++ /dev/null\n@@ -1,23 +0,0 @@\n-<html>\n-    <head>\n-    <style>\n-    body {\n-        display:block;\n-        background: url("E:\\Github\\Flask\\images.png"); \n-    }\n-    h1{\n-        text-align: center;\n-        text-decoration: underline;\n-        text-transform: uppercase;\n-    }\n-    </style>\n-    <title>Face Recognition</title>\n-    </head>\n-    <body>\n-    \n-    <h1 align = "center">Face Recognition</h1>\n-    <p align ="center">\n-    <img src="{{ url_for(\'video_feed\') }}">\n-    </p>\n-    </body>\n-</html>\n\\ No newline at end of file\ndiff --git a/utils.py b/utils.py\ndeleted file mode 100644\nindex 64b57d9..0000000\n--- a/utils.py\n+++ /dev/null\n@@ -1,156 +0,0 @@\n-from datetime import datetime\n-from PIL import Image\n-import numpy as np\n-import matplotlib.pyplot as plt\n-plt.switch_backend(\'agg\')\n-import io\n-from torchvision import transforms as trans\n-from data.data_pipe import de_preprocess\n-import torch\n-from model import l2_norm\n-import pdb\n-import cv2\n-\n-def separate_bn_paras(modules):\n-    if not isinstance(modules, list):\n-        modules = [*modules.modules()]\n-    paras_only_bn = []\n-    paras_wo_bn = []\n-    for layer in modules:\n-        if \'model\' in str(layer.__class__):\n-            continue\n-        if \'container\' in str(layer.__class__):\n-            continue\n-        else:\n-            if \'batchnorm\' in str(layer.__class__):\n-                paras_only_bn.extend([*layer.parameters()])\n-            else:\n-                paras_wo_bn.extend([*layer.parameters()])\n-    return paras_only_bn, paras_wo_bn\n-\n-def prepare_facebank(conf, model, mtcnn, tta = True):\n-    model.eval()\n-    embeddings =  []\n-    names = [\'Unknown\']\n-    for path in conf.facebank_path.iterdir():\n-        if path.is_file():\n-            continue\n-        else:\n-            embs = []\n-            for file in path.iterdir():\n-                if not file.is_file():\n-                    continue\n-                else:\n-                    try:\n-                        img = Image.open(file)\n-                    except:\n-                        continue\n-                    if img.size != (112, 112):\n-                        img = mtcnn.align(img)\n-                    with torch.no_grad():\n-                        if tta:\n-                            mirror = trans.functional.hflip(img)\n-                            emb = model(conf.test_transform(img).to(conf.device).unsqueeze(0))\n-                            emb_mirror = model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\n-                            embs.append(l2_norm(emb + emb_mirror))\n-                        else:                        \n-                            embs.append(model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\n-        if len(embs) == 0:\n-            continue\n-        embedding = torch.cat(embs).mean(0,keepdim=True)\n-        embeddings.append(embedding)\n-        names.append(path.name)\n-    embeddings = torch.cat(embeddings)\n-    names = np.array(names)\n-    torch.save(embeddings, conf.facebank_path/\'facebank.pth\')\n-    np.save(conf.facebank_path/\'names\', names)\n-    return embeddings, names\n-\n-def load_facebank(conf):\n-    embeddings = torch.load(conf.facebank_path/\'facebank.pth\')\n-    names = np.load(conf.facebank_path/\'names.npy\')\n-    return embeddings, names\n-\n-def face_reader(conf, conn, flag, boxes_arr, result_arr, learner, mtcnn, targets, tta):\n-    while True:\n-        try:\n-            image = conn.recv()\n-        except:\n-            continue\n-        try:            \n-            bboxes, faces = mtcnn.align_multi(image, limit=conf.face_limit)\n-        except:\n-            bboxes = []\n-            \n-        results = learner.infer(conf, faces, targets, tta)\n-        \n-        if len(bboxes) > 0:\n-            print(\'bboxes in reader : {}\'.format(bboxes))\n-            bboxes = bboxes[:,:-1] #shape:[10,4],only keep 10 highest possibiity faces\n-            bboxes = bboxes.astype(int)\n-            bboxes = bboxes + [-1,-1,1,1] # personal choice            \n-            assert bboxes.shape[0] == results.shape[0],\'bbox and faces number not same\'\n-            bboxes = bboxes.reshape([-1])\n-            for i in range(len(boxes_arr)):\n-                if i < len(bboxes):\n-                    boxes_arr[i] = bboxes[i]\n-                else:\n-                    boxes_arr[i] = 0 \n-            for i in range(len(result_arr)):\n-                if i < len(results):\n-                    result_arr[i] = results[i]\n-                else:\n-                    result_arr[i] = -1 \n-        else:\n-            for i in range(len(boxes_arr)):\n-                boxes_arr[i] = 0 # by default,it\'s all 0\n-            for i in range(len(result_arr)):\n-                result_arr[i] = -1 # by default,it\'s all -1\n-        print(\'boxes_arr \xef\xbc\x9a {}\'.format(boxes_arr[:4]))\n-        print(\'result_arr \xef\xbc\x9a {}\'.format(result_arr[:4]))\n-        flag.value = 0\n-\n-hflip = trans.Compose([\n-            de_preprocess,\n-            trans.ToPILImage(),\n-            trans.functional.hflip,\n-            trans.ToTensor(),\n-            trans.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n-        ])\n-\n-def hflip_batch(imgs_tensor):\n-    hfliped_imgs = torch.empty_like(imgs_tensor)\n-    for i, img_ten in enumerate(imgs_tensor):\n-        hfliped_imgs[i] = hflip(img_ten)\n-    return hfliped_imgs\n-\n-def get_time():\n-    return (str(datetime.now())[:-10]).replace(\' \',\'-\').replace(\':\',\'-\')\n-\n-def gen_plot(fpr, tpr):\n-    """Create a pyplot plot and save to buffer."""\n-    plt.figure()\n-    plt.xlabel("FPR", fontsize=14)\n-    plt.ylabel("TPR", fontsize=14)\n-    plt.title("ROC Curve", fontsize=14)\n-    plot = plt.plot(fpr, tpr, linewidth=2)\n-    buf = io.BytesIO()\n-    plt.savefig(buf, format=\'jpeg\')\n-    buf.seek(0)\n-    plt.close()\n-    return buf\n-\n-def draw_box_name(bbox,name,frame):\n-    if name == "Unknown":\n-        frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,000),6)\n-    else:\n-        frame = cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(0,0,000),6)\n-        frame = cv2.putText(frame,\n-                        name,\n-                        (bbox[0],bbox[1]), \n-                        cv2.FONT_HERSHEY_SIMPLEX, \n-                        2,\n-                        (0,255,0),\n-                        3,\n-                        cv2.LINE_AA)\n-    return frame\ndiff --git a/verifacation.py b/verifacation.py\ndeleted file mode 100644\nindex ff7d5ed..0000000\n--- a/verifacation.py\n+++ /dev/null\n@@ -1,170 +0,0 @@\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset\n-"""\n-\n-# MIT License\n-#\n-# Copyright (c) 2016 David Sandberg\n-#\n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-#\n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-#\n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-import numpy as np\n-from sklearn.model_selection import KFold\n-from sklearn.decomposition import PCA\n-import sklearn\n-from scipy import interpolate\n-import datetime\n-import mxnet as mx\n-\n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, pca=0):\n-    assert (embeddings1.shape[0] == embeddings2.shape[0])\n-    assert (embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-\n-    tprs = np.zeros((nrof_folds, nrof_thresholds))\n-    fprs = np.zeros((nrof_folds, nrof_thresholds))\n-    accuracy = np.zeros((nrof_folds))\n-    best_thresholds = np.zeros((nrof_folds))\n-    indices = np.arange(nrof_pairs)\n-    # print(\'pca\', pca)\n-\n-    if pca == 0:\n-        diff = np.subtract(embeddings1, embeddings2)\n-        dist = np.sum(np.square(diff), 1)\n-\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        # print(\'train_set\', train_set)\n-        # print(\'test_set\', test_set)\n-        if pca > 0:\n-            print(\'doing pca on\', fold_idx)\n-            embed1_train = embeddings1[train_set]\n-            embed2_train = embeddings2[train_set]\n-            _embed_train = np.concatenate((embed1_train, embed2_train), axis=0)\n-            # print(_embed_train.shape)\n-            pca_model = PCA(n_components=pca)\n-            pca_model.fit(_embed_train)\n-            embed1 = pca_model.transform(embeddings1)\n-            embed2 = pca_model.transform(embeddings2)\n-            embed1 = sklearn.preprocessing.normalize(embed1)\n-            embed2 = sklearn.preprocessing.normalize(embed2)\n-            # print(embed1.shape, embed2.shape)\n-            diff = np.subtract(embed1, embed2)\n-            dist = np.sum(np.square(diff), 1)\n-\n-        # Find the best threshold for the fold\n-        acc_train = np.zeros((nrof_thresholds))\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n-        best_threshold_index = np.argmax(acc_train)\n-#         print(\'best_threshold_index\', best_threshold_index, acc_train[best_threshold_index])\n-        best_thresholds[fold_idx] = thresholds[best_threshold_index]\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            tprs[fold_idx, threshold_idx], fprs[fold_idx, threshold_idx], _ = calculate_accuracy(threshold,\n-                                                                                                 dist[test_set],\n-                                                                                                 actual_issame[\n-                                                                                                     test_set])\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set],\n-                                                      actual_issame[test_set])\n-\n-    tpr = np.mean(tprs, 0)\n-    fpr = np.mean(fprs, 0)\n-    return tpr, fpr, accuracy, best_thresholds\n-\n-\n-def calculate_accuracy(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n-\n-    tpr = 0 if (tp + fn == 0) else float(tp) / float(tp + fn)\n-    fpr = 0 if (fp + tn == 0) else float(fp) / float(fp + tn)\n-    acc = float(tp + tn) / dist.size\n-    return tpr, fpr, acc\n-\n-\n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10):\n-    \'\'\'\n-    Copy from [insightface](https://github.com/deepinsight/insightface)\n-    :param thresholds:\n-    :param embeddings1:\n-    :param embeddings2:\n-    :param actual_issame:\n-    :param far_target:\n-    :param nrof_folds:\n-    :return:\n-    \'\'\'\n-    assert (embeddings1.shape[0] == embeddings2.shape[0])\n-    assert (embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-\n-    val = np.zeros(nrof_folds)\n-    far = np.zeros(nrof_folds)\n-\n-    diff = np.subtract(embeddings1, embeddings2)\n-    dist = np.sum(np.square(diff), 1)\n-    indices = np.arange(nrof_pairs)\n-\n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-\n-        # Find the threshold that gives FAR = far_target\n-        far_train = np.zeros(nrof_thresholds)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n-        if np.max(far_train) >= far_target:\n-            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n-            threshold = f(far_target)\n-        else:\n-            threshold = 0.0\n-\n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n-\n-    val_mean = np.mean(val)\n-    far_mean = np.mean(far)\n-    val_std = np.std(val)\n-    return val_mean, val_std, far_mean\n-\n-\n-def calculate_val_far(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    n_same = np.sum(actual_issame)\n-    n_diff = np.sum(np.logical_not(actual_issame))\n-    val = float(true_accept) / float(n_same)\n-    far = float(false_accept) / float(n_diff)\n-    return val, far\n-\n-\n-def evaluate(embeddings, actual_issame, nrof_folds=10, pca=0):\n-    # Calculate evaluation metrics\n-    thresholds = np.arange(0, 4, 0.01)\n-    embeddings1 = embeddings[0::2]\n-    embeddings2 = embeddings[1::2]\n-    tpr, fpr, accuracy, best_thresholds = calculate_roc(thresholds, embeddings1, embeddings2,\n-                                       np.asarray(actual_issame), nrof_folds=nrof_folds, pca=pca)\n-#     thresholds = np.arange(0, 4, 0.001)\n-#     val, val_std, far = calculate_val(thresholds, embeddings1, embeddings2,\n-#                                       np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds)\n-#     return tpr, fpr, accuracy, best_thresholds, val, val_std, far\n-    return tpr, fpr, accuracy, best_thresholds\n\\ No newline at end of file\ndiff --git a/work_space/history b/work_space/history\ndeleted file mode 100644\nindex 30c2973..0000000\n--- a/work_space/history\n+++ /dev/null\n@@ -1 +0,0 @@\n-/home/f/learning/face_studio/history\n\\ No newline at end of file\ndiff --git a/work_space/log b/work_space/log\ndeleted file mode 100644\nindex 2b970ae..0000000\n--- a/work_space/log\n+++ /dev/null\n@@ -1 +0,0 @@\n-/home/f/learning/face_studio/log\n\\ No newline at end of file\ndiff --git a/work_space/model/model_ir_se50.pth b/work_space/model/model_ir_se50.pth\ndeleted file mode 100644\nindex d3a030d..0000000\n--- a/work_space/model/model_ir_se50.pth\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-version https://git-lfs.github.com/spec/v1\n-oid sha256:a035c768259b98ab1ce0e646312f48b9e1e218197a0f80ac6765e88f8b6ddf28\n-size 175367323\ndiff --git a/work_space/save/model_final.pth b/work_space/save/model_final.pth\ndeleted file mode 100644\nindex d3a030d..0000000\n--- a/work_space/save/model_final.pth\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-version https://git-lfs.github.com/spec/v1\n-oid sha256:a035c768259b98ab1ce0e646312f48b9e1e218197a0f80ac6765e88f8b6ddf28\n-size 175367323'